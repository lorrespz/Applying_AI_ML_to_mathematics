{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9cb2ab",
   "metadata": {
    "papermill": {
     "duration": 0.007952,
     "end_time": "2024-03-27T17:09:26.607739",
     "exception": false,
     "start_time": "2024-03-27T17:09:26.599787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CICY4:CNN-GRU hybrid for 4 Hodge numbers\n",
    "\n",
    "- V1: Use ReduceLROnPlateau with GRU block (20, 64, 6, 4), train loss ~ 9.0\n",
    "- V2: Use OneCycleLR: train loss ~ 900 (bad result)\n",
    "- V3 Use CosineAnnealingLR: train loss ~ 60 (bad result)\n",
    "- V4: Use ReduceLROnPlateau again, but with higher max LR (bad result)\n",
    "- V5/V6: Change the GRU block architecture to have fewer layers, but with larger size for the hidden layer (100 -> 128), increase the size of the dense layers. Use ReduceLROnPlateau with max learning rate of 0.1, min LR of 1e-6. Very good result obtained: (81, 69, 48, 13) accuracy for (h11, h21, h31, h22). Train loss ~ 3.0, test loss ~ 6.0\n",
    "- V7/V8: Increase the hidden size of the GRU block from 128->256 and reduce the num_layer from 4-> 2. Same learning parameters as V5. \n",
    "- V9: Change the CNN block to have filters [128,128,64], GRU block is (20,256,1,4)\n",
    "\n",
    "For GRU block, the standalone best model so far has the parameters (in_channels, hidden_size, num_layers, outputs) =  (20, 64, 6, 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad7df39",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:26.624034Z",
     "iopub.status.busy": "2024-03-27T17:09:26.623731Z",
     "iopub.status.idle": "2024-03-27T17:09:32.391788Z",
     "shell.execute_reply": "2024-03-27T17:09:32.390802Z"
    },
    "papermill": {
     "duration": 5.77894,
     "end_time": "2024-03-27T17:09:32.394209",
     "exception": false,
     "start_time": "2024-03-27T17:09:26.615269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bda677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:32.410875Z",
     "iopub.status.busy": "2024-03-27T17:09:32.410363Z",
     "iopub.status.idle": "2024-03-27T17:09:49.645818Z",
     "shell.execute_reply": "2024-03-27T17:09:49.644845Z"
    },
    "papermill": {
     "duration": 17.246027,
     "end_time": "2024-03-27T17:09:49.648028",
     "exception": false,
     "start_time": "2024-03-27T17:09:32.402001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((921497, 16, 20), (921497, 4), (921497,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "path = '/kaggle/input/calabi-yau-cicy-4-folds/'\n",
    "\n",
    "conf = np.load('/kaggle/input/calabi-yau-cicy-4-folds/conf.npy')\n",
    "hodge = np.load(os.path.join(path, 'hodge.npy'))\n",
    "direct = np.load(os.path.join(path, 'direct.npy'))\n",
    "conf.shape, hodge.shape, direct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a344f02f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:49.664904Z",
     "iopub.status.busy": "2024-03-27T17:09:49.664608Z",
     "iopub.status.idle": "2024-03-27T17:09:49.668858Z",
     "shell.execute_reply": "2024-03-27T17:09:49.668007Z"
    },
    "papermill": {
     "duration": 0.014845,
     "end_time": "2024-03-27T17:09:49.670823",
     "exception": false,
     "start_time": "2024-03-27T17:09:49.655978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d081d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:49.686986Z",
     "iopub.status.busy": "2024-03-27T17:09:49.686708Z",
     "iopub.status.idle": "2024-03-27T17:09:49.801163Z",
     "shell.execute_reply": "2024-03-27T17:09:49.800440Z"
    },
    "papermill": {
     "duration": 0.125175,
     "end_time": "2024-03-27T17:09:49.803518",
     "exception": false,
     "start_time": "2024-03-27T17:09:49.678343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/calabi-yau-cicy-4-folds')\n",
    "from CICY4_functions import data_generator, batch_gd_scheduler,  calc_accuracy_mr, plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dce8506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:49.819800Z",
     "iopub.status.busy": "2024-03-27T17:09:49.819499Z",
     "iopub.status.idle": "2024-03-27T17:09:50.040028Z",
     "shell.execute_reply": "2024-03-27T17:09:50.039056Z"
    },
    "papermill": {
     "duration": 0.231537,
     "end_time": "2024-03-27T17:09:50.042648",
     "exception": false,
     "start_time": "2024-03-27T17:09:49.811111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test(X, y):\n",
    "    X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101, shuffle = True)\n",
    "    \n",
    "    X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "    #only need reshape if the y dimension is 1\n",
    "    #y_train = torch.from_numpy(y_train.astype(np.float32).reshape(-1, 1))\n",
    "    y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "\n",
    "    X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "    #y_test = torch.from_numpy(y_test.astype(np.float32).reshape(-1, 1))\n",
    "    y_test = torch.from_numpy(y_test.astype(np.float32))                         \n",
    "    \n",
    "    print(f'X_train shape: {X_train.shape}, \\n y_train shape:{y_train.shape},\\\n",
    "                 \\n X_test shape: {X_test.shape}, \\n y_test shape:{y_test.shape}')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8597d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:50.060618Z",
     "iopub.status.busy": "2024-03-27T17:09:50.060274Z",
     "iopub.status.idle": "2024-03-27T17:09:51.398790Z",
     "shell.execute_reply": "2024-03-27T17:09:51.397682Z"
    },
    "papermill": {
     "duration": 1.34918,
     "end_time": "2024-03-27T17:09:51.400895",
     "exception": false,
     "start_time": "2024-03-27T17:09:50.051715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([737197, 16, 20]), \n",
      " y_train shape:torch.Size([737197, 4]),                 \n",
      " X_test shape: torch.Size([184300, 16, 20]), \n",
      " y_test shape:torch.Size([184300, 4])\n"
     ]
    }
   ],
   "source": [
    "X = conf\n",
    "y = hodge\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test(X, y)\n",
    "\n",
    "train_gen = lambda: data_generator(X_train, y_train)\n",
    "test_gen = lambda: data_generator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20107e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:51.417642Z",
     "iopub.status.busy": "2024-03-27T17:09:51.417277Z",
     "iopub.status.idle": "2024-03-27T17:09:51.465955Z",
     "shell.execute_reply": "2024-03-27T17:09:51.465055Z"
    },
    "papermill": {
     "duration": 0.059044,
     "end_time": "2024-03-27T17:09:51.467871",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.408827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af245fd5",
   "metadata": {
    "papermill": {
     "duration": 0.007546,
     "end_time": "2024-03-27T17:09:51.483370",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.475824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN-RNN hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073abf9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:51.499617Z",
     "iopub.status.busy": "2024-03-27T17:09:51.499303Z",
     "iopub.status.idle": "2024-03-27T17:09:51.506752Z",
     "shell.execute_reply": "2024-03-27T17:09:51.505903Z"
    },
    "papermill": {
     "duration": 0.017866,
     "end_time": "2024-03-27T17:09:51.508715",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.490849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################### CNN ###############################\n",
    "class CNN_block(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,128, 2, 1)\n",
    "        #self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128,128, 2, 1)\n",
    "        #self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.mxpool = nn.MaxPool2d(2,2)\n",
    "        self.conv3 = nn.Conv2d(128,64, 2, 1)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.conv_total = nn.Sequential(\n",
    "            self.conv1,\n",
    "            #self.bn1,\n",
    "            self.mxpool,\n",
    "            #self.bn2,\n",
    "            self.conv2,\n",
    "            self.mxpool,\n",
    "            self.conv3,\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv_total(x))\n",
    "        #reshape is the same as flat(x)\n",
    "        #x = x.reshape(x.shape[0], -1)\n",
    "        x = self.flat(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0865b17c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:51.525127Z",
     "iopub.status.busy": "2024-03-27T17:09:51.524852Z",
     "iopub.status.idle": "2024-03-27T17:09:51.714470Z",
     "shell.execute_reply": "2024-03-27T17:09:51.713534Z"
    },
    "papermill": {
     "duration": 0.200235,
     "end_time": "2024-03-27T17:09:51.716544",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.516309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_block(\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (mxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (conv_total): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_block = CNN_block()\n",
    "cnn_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d83c87a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:51.733930Z",
     "iopub.status.busy": "2024-03-27T17:09:51.733644Z",
     "iopub.status.idle": "2024-03-27T17:09:51.741191Z",
     "shell.execute_reply": "2024-03-27T17:09:51.740211Z"
    },
    "papermill": {
     "duration": 0.018715,
     "end_time": "2024-03-27T17:09:51.743360",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.724645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN_block(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):\n",
    "        super(RNN_block,self).__init__()\n",
    "        self.D = n_inputs\n",
    "        self.M = n_hidden\n",
    "        self.K = n_outputs\n",
    "        self.L = n_rnnlayers        \n",
    "        #self.lstm = nn.LSTM(input_size = self.D,\n",
    "        #                   hidden_size = self.M,\n",
    "        #                   num_layers = self.L,\n",
    "        #                   batch_first = True)    \n",
    "        self.gru = nn.GRU(input_size = self.D,\n",
    "                           hidden_size = self.M,\n",
    "                           num_layers = self.L,\n",
    "                           batch_first = True)\n",
    "        #self.fc1 = nn.Linear(self.M, 128)\n",
    "        #self.fc2 = nn.Linear(128, self.K)\n",
    "    def forward(self, X):\n",
    "        #input X is NxTxD\n",
    "        #initial hidden states\n",
    "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        #c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        #get LSTM unit output:\n",
    "        #output is NxTxM\n",
    "        #out, _ = self.lstm(X, (h0,c0))\n",
    "        out, _ = self.gru(X, h0)   \n",
    "        #we only want h(T) at the final time step\n",
    "        # output is now of shape (N, M)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56fed344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:51.760304Z",
     "iopub.status.busy": "2024-03-27T17:09:51.759816Z",
     "iopub.status.idle": "2024-03-27T17:09:51.891201Z",
     "shell.execute_reply": "2024-03-27T17:09:51.890277Z"
    },
    "papermill": {
     "duration": 0.142076,
     "end_time": "2024-03-27T17:09:51.893317",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.751241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_block(\n",
       "  (gru): GRU(20, 300, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gru_block = RNN_block(20, 64, 6, 4)\n",
    "#gru_block = RNN_block(20, 128, 4, 4)\n",
    "gru_block = RNN_block(20, 300, 1, 4)\n",
    "gru_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe674ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:51.910708Z",
     "iopub.status.busy": "2024-03-27T17:09:51.910375Z",
     "iopub.status.idle": "2024-03-27T17:09:51.917803Z",
     "shell.execute_reply": "2024-03-27T17:09:51.916949Z"
    },
    "papermill": {
     "duration": 0.018533,
     "end_time": "2024-03-27T17:09:51.919827",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.901294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_RNN_hybrid(nn.Module):\n",
    "    def __init__(self, cnn_block, rnn_block, feat_vec_size):\n",
    "        super(CNN_RNN_hybrid, self).__init__()\n",
    "        self.cnn_block = cnn_block\n",
    "        self.rnn_block = rnn_block\n",
    "        self.feat_vec_size = feat_vec_size\n",
    "        self.fc1 = nn.Linear(self.feat_vec_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #output of cnn block is (N,384)\n",
    "        x1 = x.view(-1,1, 16,20)\n",
    "        x1 = self.cnn_block(x1)\n",
    "        #output of rnn block is (N,M = 64)\n",
    "        x2 = self.rnn_block(x)\n",
    "        #concatenate the 2 outputs to produce a feat vec (N, M+384)\n",
    "        xx = torch.cat([x1, x2], dim = 1)\n",
    "        # pass through linear layers\n",
    "        xx = self.fc1(xx)\n",
    "        #final output is 4\n",
    "        xx = self.fc2(xx)\n",
    "        \n",
    "        return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce593e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:51.936623Z",
     "iopub.status.busy": "2024-03-27T17:09:51.936324Z",
     "iopub.status.idle": "2024-03-27T17:09:51.951308Z",
     "shell.execute_reply": "2024-03-27T17:09:51.950467Z"
    },
    "papermill": {
     "duration": 0.025623,
     "end_time": "2024-03-27T17:09:51.953337",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.927714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_RNN_hybrid(\n",
       "  (cnn_block): CNN_block(\n",
       "    (conv1): Conv2d(1, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (mxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "    (conv_total): Sequential(\n",
       "      (0): Conv2d(1, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (rnn_block): RNN_block(\n",
       "    (gru): GRU(20, 300, batch_first=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=684, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_RNN_hybrid(cnn_block, gru_block, 300+384)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29fa2c80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:51.970719Z",
     "iopub.status.busy": "2024-03-27T17:09:51.970020Z",
     "iopub.status.idle": "2024-03-27T17:09:51.975109Z",
     "shell.execute_reply": "2024-03-27T17:09:51.974260Z"
    },
    "papermill": {
     "duration": 0.015788,
     "end_time": "2024-03-27T17:09:51.977083",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.961295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094476\n"
     ]
    }
   ],
   "source": [
    "#count the number of parameters in the model\n",
    "params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "print(sum(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b524e1d",
   "metadata": {
    "papermill": {
     "duration": 0.007997,
     "end_time": "2024-03-27T17:09:51.993477",
     "exception": false,
     "start_time": "2024-03-27T17:09:51.985480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Shape Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0e708a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:52.010384Z",
     "iopub.status.busy": "2024-03-27T17:09:52.010122Z",
     "iopub.status.idle": "2024-03-27T17:09:52.128709Z",
     "shell.execute_reply": "2024-03-27T17:09:52.127637Z"
    },
    "papermill": {
     "duration": 0.12938,
     "end_time": "2024-03-27T17:09:52.130727",
     "exception": false,
     "start_time": "2024-03-27T17:09:52.001347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 16, 20])\n",
      "\n",
      "Shape of the image after passing through the GRU(20, 300, batch_first=True): \n",
      " torch.Size([1, 16, 300])\n",
      "\n",
      "The final output shape is torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "y = y_train[0].to(device)\n",
    "print('RNN BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "h0 = torch.zeros(gru_block.L, X.size(0), gru_block.M).to(device)\n",
    "#c0 = torch.zeros(gru_block.L, X.size(0), gru_block.M).to(device)\n",
    "X, _ = gru_block.gru(X, h0)\n",
    "print(f'Shape of the image after passing through the {gru_block.gru}: \\n {X.shape}\\n')\n",
    "\n",
    "#get only the h(T) at the last time step\n",
    "Xg = X[:, -1, :]\n",
    "print(f'The final output shape is {Xg.shape}')\n",
    "#print(X)\n",
    "#print(f'Target: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "446c51fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:52.148533Z",
     "iopub.status.busy": "2024-03-27T17:09:52.148241Z",
     "iopub.status.idle": "2024-03-27T17:09:52.560133Z",
     "shell.execute_reply": "2024-03-27T17:09:52.558801Z"
    },
    "papermill": {
     "duration": 0.423109,
     "end_time": "2024-03-27T17:09:52.562239",
     "exception": false,
     "start_time": "2024-03-27T17:09:52.139130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 1, 16, 20])\n",
      "\n",
      "Original shape of the image after passing through Sequential(\n",
      "  (0): Conv2d(1, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "): \n",
      " torch.Size([1, 64, 2, 3])\n",
      "\n",
      "Original shape of the image after passing through Flatten(start_dim=1, end_dim=-1): \n",
      " torch.Size([1, 384])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "print('CNN BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "X = cnn_block.conv_total(X)\n",
    "print(f'Original shape of the image after passing through {cnn_block.conv_total}: \\n {X.shape}\\n')\n",
    "\n",
    "#X = X.reshape(X.shape[0], -1)\n",
    "#print(f'After reshaping: {X.shape}') [1,384]\n",
    "Xc = cnn_block.flat(X)\n",
    "print(f'Original shape of the image after passing through {cnn_block.flat}: \\n {Xc.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "220f4dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:52.580918Z",
     "iopub.status.busy": "2024-03-27T17:09:52.580622Z",
     "iopub.status.idle": "2024-03-27T17:09:52.608520Z",
     "shell.execute_reply": "2024-03-27T17:09:52.607613Z"
    },
    "papermill": {
     "duration": 0.03939,
     "end_time": "2024-03-27T17:09:52.610556",
     "exception": false,
     "start_time": "2024-03-27T17:09:52.571166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 300]), torch.Size([1, 384]), torch.Size([1, 684]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat = torch.cat([Xc, Xg], dim=1) \n",
    "Xg.shape, Xc.shape, X_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ad4927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:52.629104Z",
     "iopub.status.busy": "2024-03-27T17:09:52.628839Z",
     "iopub.status.idle": "2024-03-27T17:09:52.678307Z",
     "shell.execute_reply": "2024-03-27T17:09:52.677228Z"
    },
    "papermill": {
     "duration": 0.061021,
     "end_time": "2024-03-27T17:09:52.680218",
     "exception": false,
     "start_time": "2024-03-27T17:09:52.619197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-RNN HYBRID BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 16, 20])\n",
      "\n",
      "Shape  after passing through the entire network: \n",
      " torch.Size([1, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "print('CNN-RNN HYBRID BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "X = model(X)\n",
    "print(f'Shape  after passing through the entire network: \\n {X.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa2891",
   "metadata": {
    "papermill": {
     "duration": 0.008237,
     "end_time": "2024-03-27T17:09:52.697327",
     "exception": false,
     "start_time": "2024-03-27T17:09:52.689090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train loop with scheduler\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html\n",
    "\n",
    "https://residentmario.github.io/pytorch-training-performance-guide/lr-sched-and-optim.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3fcb02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:52.715640Z",
     "iopub.status.busy": "2024-03-27T17:09:52.715328Z",
     "iopub.status.idle": "2024-03-27T17:09:55.196986Z",
     "shell.execute_reply": "2024-03-27T17:09:55.196154Z"
    },
    "papermill": {
     "duration": 2.493437,
     "end_time": "2024-03-27T17:09:55.199300",
     "exception": false,
     "start_time": "2024-03-27T17:09:52.705863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epoch = 300\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 128\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.01)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', min_lr = 1e-7)\n",
    "#scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00263314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T17:09:55.218449Z",
     "iopub.status.busy": "2024-03-27T17:09:55.218007Z",
     "iopub.status.idle": "2024-03-27T20:05:00.942639Z",
     "shell.execute_reply": "2024-03-27T20:05:00.941640Z"
    },
    "papermill": {
     "duration": 10505.77047,
     "end_time": "2024-03-27T20:05:00.978892",
     "exception": false,
     "start_time": "2024-03-27T17:09:55.208422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300, train loss:  570.7924          test_loss:  476.1308, duration: 0:00:35.771021,           learning rate: (0.01, 0.01)\n",
      "Epoch: 2/300, train loss:  358.2353          test_loss:  237.9132, duration: 0:00:35.293875,           learning rate: (0.01, 0.01)\n",
      "Epoch: 3/300, train loss:  197.3903          test_loss:  154.0597, duration: 0:00:35.217903,           learning rate: (0.01, 0.01)\n",
      "Epoch: 4/300, train loss:  140.6326          test_loss:  114.3352, duration: 0:00:35.182976,           learning rate: (0.01, 0.01)\n",
      "Epoch: 5/300, train loss:  116.4243          test_loss:  111.1743, duration: 0:00:35.182532,           learning rate: (0.01, 0.01)\n",
      "Epoch: 6/300, train loss:  101.2093          test_loss:  104.9386, duration: 0:00:35.289836,           learning rate: (0.01, 0.01)\n",
      "Epoch: 7/300, train loss:  101.8016          test_loss:  114.2211, duration: 0:00:35.099054,           learning rate: (0.01, 0.01)\n",
      "Epoch: 8/300, train loss:  86.7113          test_loss:  178.8073, duration: 0:00:35.061486,           learning rate: (0.01, 0.01)\n",
      "Epoch: 9/300, train loss:  116508.4780          test_loss:  521.9430, duration: 0:00:35.099127,           learning rate: (0.01, 0.01)\n",
      "Epoch: 10/300, train loss:  359.2243          test_loss:  235.2400, duration: 0:00:34.907173,           learning rate: (0.01, 0.01)\n",
      "Epoch: 11/300, train loss:  184.6080          test_loss:  166.7266, duration: 0:00:34.964568,           learning rate: (0.01, 0.01)\n",
      "Epoch: 12/300, train loss:  136.0059          test_loss:  126.2090, duration: 0:00:34.951505,           learning rate: (0.01, 0.01)\n",
      "Epoch: 13/300, train loss:  107.1986          test_loss:  126.7274, duration: 0:00:34.958632,           learning rate: (0.01, 0.01)\n",
      "Epoch: 14/300, train loss:  95.4925          test_loss:  78.0333, duration: 0:00:34.857401,           learning rate: (0.01, 0.01)\n",
      "Epoch: 15/300, train loss:  87.2709          test_loss:  68.6368, duration: 0:00:35.113981,           learning rate: (0.01, 0.01)\n",
      "Epoch: 16/300, train loss:  20314895.4285          test_loss:  1222.8472, duration: 0:00:34.895344,           learning rate: (0.01, 0.01)\n",
      "Epoch: 17/300, train loss:  657.7472          test_loss:  538.5688, duration: 0:00:35.039817,           learning rate: (0.01, 0.01)\n",
      "Epoch: 18/300, train loss:  462.6261          test_loss:  384.6297, duration: 0:00:34.908506,           learning rate: (0.01, 0.01)\n",
      "Epoch: 19/300, train loss:  283.7076          test_loss:  158.1427, duration: 0:00:35.016304,           learning rate: (0.01, 0.01)\n",
      "Epoch: 20/300, train loss:  163.6493          test_loss:  188.0658, duration: 0:00:35.105029,           learning rate: (0.01, 0.01)\n",
      "Epoch: 21/300, train loss:  111.9832          test_loss:  72.0393, duration: 0:00:35.093406,           learning rate: (0.01, 0.01)\n",
      "Epoch: 22/300, train loss:  101.2299          test_loss:  124.3714, duration: 0:00:35.067915,           learning rate: (0.01, 0.01)\n",
      "Epoch: 23/300, train loss:  97.0793          test_loss:  61.2450, duration: 0:00:35.103349,           learning rate: (0.01, 0.01)\n",
      "Epoch: 24/300, train loss:  90.2285          test_loss:  69.0841, duration: 0:00:35.022008,           learning rate: (0.01, 0.01)\n",
      "Epoch: 25/300, train loss:  76.2016          test_loss:  129.9317, duration: 0:00:34.976153,           learning rate: (0.01, 0.01)\n",
      "Epoch: 26/300, train loss:  80.2865          test_loss:  63.6428, duration: 0:00:34.958295,           learning rate: (0.01, 0.01)\n",
      "Epoch: 27/300, train loss:  79.8545          test_loss:  69.9407, duration: 0:00:34.911428,           learning rate: (0.01, 0.01)\n",
      "Epoch: 28/300, train loss:  93.8746          test_loss:  62.4240, duration: 0:00:34.858827,           learning rate: (0.01, 0.01)\n",
      "Epoch: 29/300, train loss:  70.7433          test_loss:  48.5829, duration: 0:00:35.084972,           learning rate: (0.01, 0.01)\n",
      "Epoch: 30/300, train loss:  71.4694          test_loss:  51.2656, duration: 0:00:35.067732,           learning rate: (0.01, 0.01)\n",
      "Epoch: 31/300, train loss:  72.7909          test_loss:  62.6535, duration: 0:00:34.995793,           learning rate: (0.01, 0.01)\n",
      "Epoch: 32/300, train loss:  69.9870          test_loss:  48.5189, duration: 0:00:35.144715,           learning rate: (0.01, 0.01)\n",
      "Epoch: 33/300, train loss:  75.8336          test_loss:  67.1742, duration: 0:00:35.090938,           learning rate: (0.01, 0.01)\n",
      "Epoch: 34/300, train loss:  67.2529          test_loss:  58.0629, duration: 0:00:34.902844,           learning rate: (0.01, 0.01)\n",
      "Epoch: 35/300, train loss:  68.0713          test_loss:  45.2555, duration: 0:00:34.947761,           learning rate: (0.01, 0.01)\n",
      "Epoch: 36/300, train loss:  64.3625          test_loss:  74.4788, duration: 0:00:35.025914,           learning rate: (0.01, 0.01)\n",
      "Epoch: 37/300, train loss:  70.3728          test_loss:  60.4296, duration: 0:00:34.999275,           learning rate: (0.01, 0.01)\n",
      "Epoch: 38/300, train loss:  72.1589          test_loss:  51.0012, duration: 0:00:35.001987,           learning rate: (0.01, 0.01)\n",
      "Epoch: 39/300, train loss:  66.4503          test_loss:  96.3198, duration: 0:00:35.023809,           learning rate: (0.01, 0.01)\n",
      "Epoch: 40/300, train loss:  70.0512          test_loss:  61.2480, duration: 0:00:35.005903,           learning rate: (0.01, 0.01)\n",
      "Epoch: 41/300, train loss:  73.0349          test_loss:  53.4493, duration: 0:00:34.944892,           learning rate: (0.01, 0.01)\n",
      "Epoch: 42/300, train loss:  72.6004          test_loss:  69.2871, duration: 0:00:34.998643,           learning rate: (0.01, 0.01)\n",
      "Epoch: 43/300, train loss:  66.0163          test_loss:  49.2635, duration: 0:00:35.016109,           learning rate: (0.01, 0.01)\n",
      "Epoch: 44/300, train loss:  64.7000          test_loss:  48.8304, duration: 0:00:34.893151,           learning rate: (0.01, 0.01)\n",
      "Epoch: 45/300, train loss:  65.2524          test_loss:  55.9266, duration: 0:00:34.938247,           learning rate: (0.01, 0.01)\n",
      "Epoch: 46/300, train loss:  74.3250          test_loss:  66.8131, duration: 0:00:34.990072,           learning rate: (0.01, 0.001)\n",
      "Epoch: 47/300, train loss:  27.2906          test_loss:  24.1803, duration: 0:00:35.012562,           learning rate: (0.001, 0.001)\n",
      "Epoch: 48/300, train loss:  22.4351          test_loss:  21.5504, duration: 0:00:35.011211,           learning rate: (0.001, 0.001)\n",
      "Epoch: 49/300, train loss:  20.3725          test_loss:  21.0978, duration: 0:00:34.997367,           learning rate: (0.001, 0.001)\n",
      "Epoch: 50/300, train loss:  19.2699          test_loss:  19.8815, duration: 0:00:35.042528,           learning rate: (0.001, 0.001)\n",
      "Epoch: 51/300, train loss:  18.4004          test_loss:  19.5554, duration: 0:00:35.009538,           learning rate: (0.001, 0.001)\n",
      "Epoch: 52/300, train loss:  17.5217          test_loss:  18.2766, duration: 0:00:35.010024,           learning rate: (0.001, 0.001)\n",
      "Epoch: 53/300, train loss:  17.3924          test_loss:  17.4967, duration: 0:00:34.975700,           learning rate: (0.001, 0.001)\n",
      "Epoch: 54/300, train loss:  16.5451          test_loss:  16.8670, duration: 0:00:34.948307,           learning rate: (0.001, 0.001)\n",
      "Epoch: 55/300, train loss:  16.1709          test_loss:  16.2883, duration: 0:00:35.040795,           learning rate: (0.001, 0.001)\n",
      "Epoch: 56/300, train loss:  15.8722          test_loss:  18.6354, duration: 0:00:35.115101,           learning rate: (0.001, 0.001)\n",
      "Epoch: 57/300, train loss:  15.3841          test_loss:  17.3750, duration: 0:00:35.048529,           learning rate: (0.001, 0.001)\n",
      "Epoch: 58/300, train loss:  15.0668          test_loss:  17.1764, duration: 0:00:35.103107,           learning rate: (0.001, 0.001)\n",
      "Epoch: 59/300, train loss:  15.2338          test_loss:  14.1780, duration: 0:00:35.225759,           learning rate: (0.001, 0.001)\n",
      "Epoch: 60/300, train loss:  14.7241          test_loss:  14.9427, duration: 0:00:34.990683,           learning rate: (0.001, 0.001)\n",
      "Epoch: 61/300, train loss:  14.3890          test_loss:  14.5805, duration: 0:00:35.001848,           learning rate: (0.001, 0.001)\n",
      "Epoch: 62/300, train loss:  14.8087          test_loss:  17.0502, duration: 0:00:34.809965,           learning rate: (0.001, 0.001)\n",
      "Epoch: 63/300, train loss:  14.6510          test_loss:  15.8383, duration: 0:00:34.831636,           learning rate: (0.001, 0.001)\n",
      "Epoch: 64/300, train loss:  13.7055          test_loss:  17.0310, duration: 0:00:34.999175,           learning rate: (0.001, 0.001)\n",
      "Epoch: 65/300, train loss:  14.0270          test_loss:  16.5217, duration: 0:00:34.979847,           learning rate: (0.001, 0.001)\n",
      "Epoch: 66/300, train loss:  13.3508          test_loss:  13.3223, duration: 0:00:35.043519,           learning rate: (0.001, 0.001)\n",
      "Epoch: 67/300, train loss:  12.9279          test_loss:  18.5637, duration: 0:00:34.933280,           learning rate: (0.001, 0.001)\n",
      "Epoch: 68/300, train loss:  12.8644          test_loss:  12.2509, duration: 0:00:34.902720,           learning rate: (0.001, 0.001)\n",
      "Epoch: 69/300, train loss:  12.9439          test_loss:  12.2522, duration: 0:00:35.014480,           learning rate: (0.001, 0.001)\n",
      "Epoch: 70/300, train loss:  13.9986          test_loss:  12.3545, duration: 0:00:35.000202,           learning rate: (0.001, 0.001)\n",
      "Epoch: 71/300, train loss:  13.9311          test_loss:  20.1108, duration: 0:00:35.004221,           learning rate: (0.001, 0.001)\n",
      "Epoch: 72/300, train loss:  12.5776          test_loss:  11.7542, duration: 0:00:34.882483,           learning rate: (0.001, 0.001)\n",
      "Epoch: 73/300, train loss:  13.8348          test_loss:  13.5028, duration: 0:00:35.015444,           learning rate: (0.001, 0.001)\n",
      "Epoch: 74/300, train loss:  13.3414          test_loss:  12.5928, duration: 0:00:34.922754,           learning rate: (0.001, 0.001)\n",
      "Epoch: 75/300, train loss:  12.3440          test_loss:  12.6344, duration: 0:00:35.139576,           learning rate: (0.001, 0.001)\n",
      "Epoch: 76/300, train loss:  11.7283          test_loss:  14.7913, duration: 0:00:34.848182,           learning rate: (0.001, 0.001)\n",
      "Epoch: 77/300, train loss:  11.9922          test_loss:  12.1880, duration: 0:00:34.887717,           learning rate: (0.001, 0.001)\n",
      "Epoch: 78/300, train loss:  13.2787          test_loss:  12.3897, duration: 0:00:34.887662,           learning rate: (0.001, 0.001)\n",
      "Epoch: 79/300, train loss:  11.3070          test_loss:  10.8711, duration: 0:00:34.954883,           learning rate: (0.001, 0.001)\n",
      "Epoch: 80/300, train loss:  11.3739          test_loss:  11.4012, duration: 0:00:34.931215,           learning rate: (0.001, 0.001)\n",
      "Epoch: 81/300, train loss:  10.9285          test_loss:  13.3418, duration: 0:00:34.873578,           learning rate: (0.001, 0.001)\n",
      "Epoch: 82/300, train loss:  12.0489          test_loss:  20.2158, duration: 0:00:34.957454,           learning rate: (0.001, 0.001)\n",
      "Epoch: 83/300, train loss:  11.3636          test_loss:  16.0284, duration: 0:00:35.027102,           learning rate: (0.001, 0.001)\n",
      "Epoch: 84/300, train loss:  13.1996          test_loss:  10.6176, duration: 0:00:34.997548,           learning rate: (0.001, 0.001)\n",
      "Epoch: 85/300, train loss:  11.8164          test_loss:  12.6861, duration: 0:00:34.947739,           learning rate: (0.001, 0.001)\n",
      "Epoch: 86/300, train loss:  10.6251          test_loss:  12.0879, duration: 0:00:34.957677,           learning rate: (0.001, 0.001)\n",
      "Epoch: 87/300, train loss:  11.1033          test_loss:  40.7782, duration: 0:00:35.056098,           learning rate: (0.001, 0.001)\n",
      "Epoch: 88/300, train loss:  13.9888          test_loss:  13.6297, duration: 0:00:34.871286,           learning rate: (0.001, 0.001)\n",
      "Epoch: 89/300, train loss:  10.3952          test_loss:  10.1154, duration: 0:00:34.827386,           learning rate: (0.001, 0.001)\n",
      "Epoch: 90/300, train loss:  15.1935          test_loss:  17.4996, duration: 0:00:34.957854,           learning rate: (0.001, 0.001)\n",
      "Epoch: 91/300, train loss:  11.8059          test_loss:  12.4444, duration: 0:00:34.845100,           learning rate: (0.001, 0.001)\n",
      "Epoch: 92/300, train loss:  10.9446          test_loss:  45.9902, duration: 0:00:34.987923,           learning rate: (0.001, 0.001)\n",
      "Epoch: 93/300, train loss:  11.1250          test_loss:  55.4519, duration: 0:00:35.080406,           learning rate: (0.001, 0.001)\n",
      "Epoch: 94/300, train loss:  11.0048          test_loss:  12.3019, duration: 0:00:35.086298,           learning rate: (0.001, 0.001)\n",
      "Epoch: 95/300, train loss:  11.5606          test_loss:  10.6717, duration: 0:00:34.880879,           learning rate: (0.001, 0.001)\n",
      "Epoch: 96/300, train loss:  12.6071          test_loss:  23.0293, duration: 0:00:34.882778,           learning rate: (0.001, 0.001)\n",
      "Epoch: 97/300, train loss:  10.9579          test_loss:  9.3555, duration: 0:00:35.007458,           learning rate: (0.001, 0.001)\n",
      "Epoch: 98/300, train loss:  11.4059          test_loss:  26.7080, duration: 0:00:34.780137,           learning rate: (0.001, 0.001)\n",
      "Epoch: 99/300, train loss:  10.2511          test_loss:  10.1424, duration: 0:00:34.946110,           learning rate: (0.001, 0.001)\n",
      "Epoch: 100/300, train loss:  10.2837          test_loss:  12.1209, duration: 0:00:34.940885,           learning rate: (0.001, 0.001)\n",
      "Epoch: 101/300, train loss:  12.8824          test_loss:  12.3917, duration: 0:00:35.018210,           learning rate: (0.001, 0.001)\n",
      "Epoch: 102/300, train loss:  10.4698          test_loss:  12.8726, duration: 0:00:34.936109,           learning rate: (0.001, 0.001)\n",
      "Epoch: 103/300, train loss:  14.3527          test_loss:  16.0325, duration: 0:00:35.013095,           learning rate: (0.001, 0.001)\n",
      "Epoch: 104/300, train loss:  10.0820          test_loss:  12.5114, duration: 0:00:34.942917,           learning rate: (0.001, 0.001)\n",
      "Epoch: 105/300, train loss:  10.7096          test_loss:  11.5221, duration: 0:00:34.986245,           learning rate: (0.001, 0.001)\n",
      "Epoch: 106/300, train loss:  11.7986          test_loss:  17.6246, duration: 0:00:34.893385,           learning rate: (0.001, 0.001)\n",
      "Epoch: 107/300, train loss:  9.9102          test_loss:  14.1325, duration: 0:00:34.970546,           learning rate: (0.001, 0.001)\n",
      "Epoch: 108/300, train loss:  10.3553          test_loss:  9.7411, duration: 0:00:34.965790,           learning rate: (0.001, 0.0001)\n",
      "Epoch: 109/300, train loss:  5.3850          test_loss:  6.5530, duration: 0:00:34.986426,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 110/300, train loss:  4.8468          test_loss:  6.2450, duration: 0:00:34.967358,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 111/300, train loss:  4.6369          test_loss:  6.2291, duration: 0:00:34.992527,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 112/300, train loss:  4.4953          test_loss:  5.9504, duration: 0:00:34.985612,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 113/300, train loss:  4.3905          test_loss:  5.8409, duration: 0:00:34.916494,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 114/300, train loss:  4.3020          test_loss:  5.7877, duration: 0:00:34.927576,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 115/300, train loss:  4.2281          test_loss:  5.8703, duration: 0:00:34.912532,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 116/300, train loss:  4.1718          test_loss:  5.7063, duration: 0:00:35.020108,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 117/300, train loss:  4.1125          test_loss:  5.6929, duration: 0:00:34.935190,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 118/300, train loss:  4.0607          test_loss:  5.6294, duration: 0:00:35.077237,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 119/300, train loss:  4.0156          test_loss:  5.5740, duration: 0:00:34.904991,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 120/300, train loss:  3.9679          test_loss:  5.4561, duration: 0:00:34.815783,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 121/300, train loss:  3.9280          test_loss:  5.4794, duration: 0:00:34.866403,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 122/300, train loss:  3.8817          test_loss:  5.3969, duration: 0:00:35.134442,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 123/300, train loss:  3.8482          test_loss:  5.4006, duration: 0:00:34.878005,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 124/300, train loss:  3.8158          test_loss:  5.3487, duration: 0:00:34.860205,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 125/300, train loss:  3.7789          test_loss:  5.3456, duration: 0:00:34.966729,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 126/300, train loss:  3.7475          test_loss:  5.2576, duration: 0:00:34.888084,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 127/300, train loss:  3.7252          test_loss:  5.2989, duration: 0:00:34.911830,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 128/300, train loss:  3.6873          test_loss:  5.2620, duration: 0:00:34.998621,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 129/300, train loss:  3.6620          test_loss:  5.2544, duration: 0:00:34.857208,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 130/300, train loss:  3.6387          test_loss:  5.2730, duration: 0:00:35.062415,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 131/300, train loss:  3.6105          test_loss:  5.1634, duration: 0:00:34.882496,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 132/300, train loss:  3.5830          test_loss:  5.2491, duration: 0:00:34.957568,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 133/300, train loss:  3.5617          test_loss:  5.1959, duration: 0:00:34.920815,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 134/300, train loss:  3.5412          test_loss:  5.0770, duration: 0:00:34.991648,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 135/300, train loss:  3.5172          test_loss:  5.1357, duration: 0:00:34.961985,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 136/300, train loss:  3.4924          test_loss:  5.0668, duration: 0:00:34.939770,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 137/300, train loss:  3.4765          test_loss:  5.0081, duration: 0:00:34.922771,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 138/300, train loss:  3.4531          test_loss:  4.9927, duration: 0:00:34.974321,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 139/300, train loss:  3.4316          test_loss:  5.0126, duration: 0:00:34.951434,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 140/300, train loss:  3.4143          test_loss:  4.9713, duration: 0:00:34.978134,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 141/300, train loss:  3.4016          test_loss:  4.9294, duration: 0:00:35.041162,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 142/300, train loss:  3.3818          test_loss:  4.9305, duration: 0:00:34.912143,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 143/300, train loss:  3.3605          test_loss:  5.1130, duration: 0:00:34.837703,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 144/300, train loss:  3.3503          test_loss:  4.8760, duration: 0:00:34.982925,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 145/300, train loss:  3.3257          test_loss:  4.8825, duration: 0:00:34.908766,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 146/300, train loss:  3.3146          test_loss:  4.9446, duration: 0:00:35.031943,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 147/300, train loss:  3.2984          test_loss:  4.8650, duration: 0:00:34.918699,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 148/300, train loss:  3.2789          test_loss:  4.7851, duration: 0:00:34.989668,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 149/300, train loss:  3.2689          test_loss:  4.9025, duration: 0:00:35.106508,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 150/300, train loss:  3.2492          test_loss:  4.8659, duration: 0:00:35.142026,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 151/300, train loss:  3.2447          test_loss:  4.7867, duration: 0:00:34.937553,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 152/300, train loss:  3.2302          test_loss:  4.8056, duration: 0:00:35.317387,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 153/300, train loss:  3.2157          test_loss:  4.7639, duration: 0:00:34.954586,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 154/300, train loss:  3.1983          test_loss:  5.0102, duration: 0:00:34.954440,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 155/300, train loss:  3.1887          test_loss:  4.7184, duration: 0:00:35.115424,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 156/300, train loss:  3.1798          test_loss:  4.8350, duration: 0:00:34.935471,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 157/300, train loss:  3.1619          test_loss:  4.7204, duration: 0:00:34.986822,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 158/300, train loss:  3.1543          test_loss:  4.6984, duration: 0:00:35.009193,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 159/300, train loss:  3.1385          test_loss:  4.7244, duration: 0:00:34.999840,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 160/300, train loss:  3.1283          test_loss:  4.6410, duration: 0:00:34.891787,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 161/300, train loss:  3.1168          test_loss:  4.6503, duration: 0:00:35.064375,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 162/300, train loss:  3.1114          test_loss:  4.7232, duration: 0:00:34.925437,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 163/300, train loss:  3.1020          test_loss:  4.7751, duration: 0:00:35.052540,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 164/300, train loss:  3.0889          test_loss:  4.5971, duration: 0:00:34.893609,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 165/300, train loss:  3.0774          test_loss:  4.5850, duration: 0:00:34.966830,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 166/300, train loss:  3.0673          test_loss:  4.6093, duration: 0:00:34.858472,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 167/300, train loss:  3.0565          test_loss:  4.7163, duration: 0:00:34.858653,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 168/300, train loss:  3.0485          test_loss:  4.8472, duration: 0:00:35.048049,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 169/300, train loss:  3.0398          test_loss:  4.6256, duration: 0:00:34.935985,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 170/300, train loss:  3.0309          test_loss:  4.6751, duration: 0:00:34.889113,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 171/300, train loss:  3.0191          test_loss:  4.6082, duration: 0:00:34.977554,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 172/300, train loss:  3.0189          test_loss:  4.6043, duration: 0:00:35.014644,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 173/300, train loss:  3.0109          test_loss:  4.4940, duration: 0:00:35.035891,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 174/300, train loss:  3.0051          test_loss:  4.6145, duration: 0:00:34.988230,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 175/300, train loss:  2.9882          test_loss:  4.6756, duration: 0:00:35.159748,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 176/300, train loss:  2.9946          test_loss:  12.2571, duration: 0:00:35.185407,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 177/300, train loss:  3.1140          test_loss:  4.9238, duration: 0:00:35.328800,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 178/300, train loss:  2.9732          test_loss:  4.5708, duration: 0:00:35.221742,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 179/300, train loss:  2.9736          test_loss:  4.5240, duration: 0:00:35.236807,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 180/300, train loss:  2.9551          test_loss:  4.5224, duration: 0:00:35.186879,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 181/300, train loss:  2.9614          test_loss:  4.4689, duration: 0:00:35.152618,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 182/300, train loss:  2.9437          test_loss:  4.5544, duration: 0:00:35.024823,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 183/300, train loss:  2.9390          test_loss:  4.5576, duration: 0:00:35.050916,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 184/300, train loss:  2.9301          test_loss:  4.5026, duration: 0:00:35.040283,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 185/300, train loss:  2.9263          test_loss:  4.6499, duration: 0:00:35.097373,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 186/300, train loss:  2.9339          test_loss:  4.6179, duration: 0:00:35.281213,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 187/300, train loss:  2.9199          test_loss:  4.4261, duration: 0:00:34.994228,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 188/300, train loss:  2.9053          test_loss:  4.4185, duration: 0:00:35.089969,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 189/300, train loss:  2.9039          test_loss:  4.7394, duration: 0:00:34.986168,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 190/300, train loss:  2.8905          test_loss:  4.4350, duration: 0:00:34.989345,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 191/300, train loss:  2.8851          test_loss:  4.6225, duration: 0:00:34.989985,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 192/300, train loss:  2.8915          test_loss:  4.5741, duration: 0:00:35.081887,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 193/300, train loss:  2.8822          test_loss:  4.4563, duration: 0:00:34.907577,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 194/300, train loss:  2.8775          test_loss:  4.4419, duration: 0:00:35.118513,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 195/300, train loss:  2.8672          test_loss:  4.4303, duration: 0:00:35.218867,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 196/300, train loss:  2.8647          test_loss:  4.4944, duration: 0:00:35.077268,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 197/300, train loss:  2.8875          test_loss:  4.4022, duration: 0:00:35.004595,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 198/300, train loss:  2.8531          test_loss:  4.5155, duration: 0:00:35.140836,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 199/300, train loss:  2.9376          test_loss:  4.8500, duration: 0:00:35.220000,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 200/300, train loss:  3.0245          test_loss:  4.3025, duration: 0:00:35.122037,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 201/300, train loss:  2.8578          test_loss:  4.3963, duration: 0:00:35.054460,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 202/300, train loss:  2.8467          test_loss:  4.7774, duration: 0:00:35.167495,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 203/300, train loss:  2.8466          test_loss:  4.4314, duration: 0:00:34.956692,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 204/300, train loss:  2.8301          test_loss:  4.5651, duration: 0:00:35.246165,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 205/300, train loss:  2.8422          test_loss:  4.5895, duration: 0:00:34.978820,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 206/300, train loss:  2.8284          test_loss:  4.5125, duration: 0:00:34.991324,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 207/300, train loss:  2.8302          test_loss:  4.4339, duration: 0:00:35.091465,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 208/300, train loss:  2.8306          test_loss:  4.3649, duration: 0:00:34.910055,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 209/300, train loss:  2.8143          test_loss:  4.3631, duration: 0:00:34.959376,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 210/300, train loss:  2.8107          test_loss:  4.4150, duration: 0:00:34.979448,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 211/300, train loss:  2.8051          test_loss:  4.4687, duration: 0:00:34.959900,           learning rate: (0.0001, 1e-05)\n",
      "Epoch: 212/300, train loss:  2.3686          test_loss:  4.0283, duration: 0:00:34.983565,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 213/300, train loss:  2.3364          test_loss:  4.0047, duration: 0:00:35.069806,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 214/300, train loss:  2.3287          test_loss:  4.0102, duration: 0:00:34.878562,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 215/300, train loss:  2.3243          test_loss:  3.9953, duration: 0:00:34.933561,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 216/300, train loss:  2.3217          test_loss:  3.9972, duration: 0:00:35.041639,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 217/300, train loss:  2.3188          test_loss:  4.0083, duration: 0:00:34.999166,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 218/300, train loss:  2.3166          test_loss:  4.0172, duration: 0:00:34.885604,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 219/300, train loss:  2.3149          test_loss:  4.0341, duration: 0:00:34.788836,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 220/300, train loss:  2.3121          test_loss:  4.0012, duration: 0:00:35.019688,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 221/300, train loss:  2.3115          test_loss:  4.0068, duration: 0:00:35.242734,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 222/300, train loss:  2.3091          test_loss:  4.0247, duration: 0:00:35.167548,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 223/300, train loss:  2.3079          test_loss:  4.0116, duration: 0:00:35.054842,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 224/300, train loss:  2.3074          test_loss:  4.0018, duration: 0:00:34.899619,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 225/300, train loss:  2.3053          test_loss:  3.9998, duration: 0:00:35.119728,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 226/300, train loss:  2.3048          test_loss:  4.0034, duration: 0:00:34.927911,           learning rate: (1e-05, 1.0000000000000002e-06)\n",
      "Epoch: 227/300, train loss:  2.2598          test_loss:  3.9790, duration: 0:00:34.824251,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 228/300, train loss:  2.2561          test_loss:  3.9793, duration: 0:00:34.968577,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 229/300, train loss:  2.2549          test_loss:  3.9788, duration: 0:00:35.017621,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 230/300, train loss:  2.2542          test_loss:  3.9758, duration: 0:00:35.002446,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 231/300, train loss:  2.2539          test_loss:  3.9780, duration: 0:00:34.919734,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 232/300, train loss:  2.2535          test_loss:  3.9769, duration: 0:00:34.978054,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 233/300, train loss:  2.2531          test_loss:  3.9765, duration: 0:00:34.953114,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 234/300, train loss:  2.2525          test_loss:  3.9768, duration: 0:00:34.937998,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 235/300, train loss:  2.2522          test_loss:  3.9773, duration: 0:00:35.038721,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 236/300, train loss:  2.2518          test_loss:  3.9760, duration: 0:00:34.948924,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 237/300, train loss:  2.2513          test_loss:  3.9768, duration: 0:00:35.018470,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 238/300, train loss:  2.2513          test_loss:  3.9789, duration: 0:00:35.063186,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 239/300, train loss:  2.2508          test_loss:  3.9796, duration: 0:00:34.951775,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 240/300, train loss:  2.2503          test_loss:  3.9778, duration: 0:00:35.158165,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-06)\n",
      "Epoch: 241/300, train loss:  2.2501          test_loss:  3.9799, duration: 0:00:35.293767,           learning rate: (1.0000000000000002e-06, 1.0000000000000002e-07)\n",
      "Epoch: 242/300, train loss:  2.2451          test_loss:  3.9740, duration: 0:00:34.942515,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 243/300, train loss:  2.2443          test_loss:  3.9746, duration: 0:00:34.998147,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 244/300, train loss:  2.2442          test_loss:  3.9740, duration: 0:00:35.035402,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 245/300, train loss:  2.2440          test_loss:  3.9719, duration: 0:00:35.111685,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 246/300, train loss:  2.2441          test_loss:  3.9752, duration: 0:00:34.887589,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 247/300, train loss:  2.2440          test_loss:  3.9742, duration: 0:00:34.853074,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 248/300, train loss:  2.2441          test_loss:  3.9731, duration: 0:00:35.050907,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 249/300, train loss:  2.2440          test_loss:  3.9718, duration: 0:00:34.983078,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 250/300, train loss:  2.2439          test_loss:  3.9746, duration: 0:00:35.177817,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 251/300, train loss:  2.2439          test_loss:  3.9734, duration: 0:00:34.956474,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 252/300, train loss:  2.2439          test_loss:  3.9734, duration: 0:00:35.041913,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 253/300, train loss:  2.2438          test_loss:  3.9740, duration: 0:00:34.924656,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 254/300, train loss:  2.2438          test_loss:  3.9744, duration: 0:00:34.881469,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 255/300, train loss:  2.2438          test_loss:  3.9727, duration: 0:00:34.914193,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 256/300, train loss:  2.2438          test_loss:  3.9736, duration: 0:00:34.944405,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 257/300, train loss:  2.2437          test_loss:  3.9737, duration: 0:00:35.064379,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 258/300, train loss:  2.2437          test_loss:  3.9744, duration: 0:00:35.200085,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 259/300, train loss:  2.2437          test_loss:  3.9711, duration: 0:00:35.230200,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 260/300, train loss:  2.2436          test_loss:  3.9739, duration: 0:00:35.153228,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 261/300, train loss:  2.2436          test_loss:  3.9740, duration: 0:00:35.239528,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 262/300, train loss:  2.2435          test_loss:  3.9702, duration: 0:00:35.067079,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 263/300, train loss:  2.2435          test_loss:  3.9693, duration: 0:00:35.079031,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 264/300, train loss:  2.2435          test_loss:  3.9743, duration: 0:00:35.116626,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 265/300, train loss:  2.2434          test_loss:  3.9741, duration: 0:00:35.095541,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 266/300, train loss:  2.2434          test_loss:  3.9704, duration: 0:00:35.049284,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 267/300, train loss:  2.2434          test_loss:  3.9750, duration: 0:00:34.950405,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 268/300, train loss:  2.2434          test_loss:  3.9736, duration: 0:00:35.114944,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 269/300, train loss:  2.2433          test_loss:  3.9731, duration: 0:00:34.924563,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 270/300, train loss:  2.2433          test_loss:  3.9738, duration: 0:00:35.259528,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 271/300, train loss:  2.2433          test_loss:  3.9736, duration: 0:00:35.021014,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 272/300, train loss:  2.2432          test_loss:  3.9735, duration: 0:00:35.195159,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 273/300, train loss:  2.2432          test_loss:  3.9734, duration: 0:00:35.041417,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 274/300, train loss:  2.2431          test_loss:  3.9733, duration: 0:00:34.965346,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 275/300, train loss:  2.2431          test_loss:  3.9727, duration: 0:00:35.103495,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 276/300, train loss:  2.2430          test_loss:  3.9726, duration: 0:00:34.926832,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 277/300, train loss:  2.2430          test_loss:  3.9740, duration: 0:00:35.036341,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 278/300, train loss:  2.2430          test_loss:  3.9731, duration: 0:00:35.055082,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 279/300, train loss:  2.2429          test_loss:  3.9737, duration: 0:00:35.111726,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 280/300, train loss:  2.2429          test_loss:  3.9735, duration: 0:00:35.415176,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 281/300, train loss:  2.2430          test_loss:  3.9732, duration: 0:00:35.695218,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 282/300, train loss:  2.2429          test_loss:  3.9723, duration: 0:00:35.529235,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 283/300, train loss:  2.2428          test_loss:  3.9728, duration: 0:00:35.552749,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 284/300, train loss:  2.2428          test_loss:  3.9742, duration: 0:00:35.697416,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 285/300, train loss:  2.2428          test_loss:  3.9715, duration: 0:00:34.970575,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 286/300, train loss:  2.2427          test_loss:  3.9742, duration: 0:00:34.945377,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 287/300, train loss:  2.2427          test_loss:  3.9740, duration: 0:00:35.041392,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 288/300, train loss:  2.2427          test_loss:  3.9731, duration: 0:00:34.945588,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 289/300, train loss:  2.2427          test_loss:  3.9730, duration: 0:00:35.141194,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 290/300, train loss:  2.2426          test_loss:  3.9736, duration: 0:00:34.951582,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 291/300, train loss:  2.2425          test_loss:  3.9724, duration: 0:00:34.918356,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 292/300, train loss:  2.2426          test_loss:  3.9733, duration: 0:00:34.859379,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 293/300, train loss:  2.2425          test_loss:  3.9741, duration: 0:00:34.833602,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 294/300, train loss:  2.2424          test_loss:  3.9740, duration: 0:00:35.124277,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 295/300, train loss:  2.2424          test_loss:  3.9726, duration: 0:00:35.123843,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 296/300, train loss:  2.2425          test_loss:  3.9732, duration: 0:00:35.071410,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 297/300, train loss:  2.2424          test_loss:  3.9737, duration: 0:00:34.953026,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 298/300, train loss:  2.2423          test_loss:  3.9739, duration: 0:00:35.007897,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 299/300, train loss:  2.2423          test_loss:  3.9735, duration: 0:00:35.019351,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n",
      "Epoch: 300/300, train loss:  2.2423          test_loss:  3.9730, duration: 0:00:34.919993,           learning rate: (1.0000000000000002e-07, 1.0000000000000002e-07)\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses=batch_gd_scheduler(model, criterion, optimizer, train_gen, test_gen, scheduler, \n",
    "                                             max_epoch, device,  cnn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7efc9055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T20:05:01.045408Z",
     "iopub.status.busy": "2024-03-27T20:05:01.044711Z",
     "iopub.status.idle": "2024-03-27T20:05:01.067820Z",
     "shell.execute_reply": "2024-03-27T20:05:01.066941Z"
    },
    "papermill": {
     "duration": 0.057556,
     "end_time": "2024-03-27T20:05:01.069671",
     "exception": false,
     "start_time": "2024-03-27T20:05:01.012115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570.792426</td>\n",
       "      <td>476.130835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>358.235288</td>\n",
       "      <td>237.913180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197.390255</td>\n",
       "      <td>154.059658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140.632643</td>\n",
       "      <td>114.335237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116.424261</td>\n",
       "      <td>111.174346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss   test_loss\n",
       "0  570.792426  476.130835\n",
       "1  358.235288  237.913180\n",
       "2  197.390255  154.059658\n",
       "3  140.632643  114.335237\n",
       "4  116.424261  111.174346"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "loss_dict = {'train_loss': train_losses, 'test_loss': test_losses}\n",
    "dd = pd.DataFrame(loss_dict)\n",
    "dd.to_csv('loss_dict.csv', index = False)\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "423f6378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T20:05:01.133919Z",
     "iopub.status.busy": "2024-03-27T20:05:01.133331Z",
     "iopub.status.idle": "2024-03-27T20:05:01.429230Z",
     "shell.execute_reply": "2024-03-27T20:05:01.428066Z"
    },
    "papermill": {
     "duration": 0.330646,
     "end_time": "2024-03-27T20:05:01.431873",
     "exception": false,
     "start_time": "2024-03-27T20:05:01.101227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03ad6fda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T20:05:01.499172Z",
     "iopub.status.busy": "2024-03-27T20:05:01.498861Z",
     "iopub.status.idle": "2024-03-27T20:05:15.801663Z",
     "shell.execute_reply": "2024-03-27T20:05:15.800461Z"
    },
    "papermill": {
     "duration": 14.338619,
     "end_time": "2024-03-27T20:05:15.803696",
     "exception": false,
     "start_time": "2024-03-27T20:05:01.465077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for h11:0.7863, Test accuracy for h11: 0.7833\n",
      "Train accuracy for h21:0.7084, Test accuracy for h21: 0.7036\n",
      "Train accuracy for h31:0.5202, Test accuracy for h31: 0.4830\n",
      "Train accuracy for h22:0.1926, Test accuracy for h22: 0.1736\n"
     ]
    }
   ],
   "source": [
    "train_acc, test_acc = calc_accuracy_mr(model, train_gen , test_gen, device = device, cnn= False)\n",
    "print(f'Train accuracy for h11:{train_acc[0]:.4f}, Test accuracy for h11: {test_acc[0]:.4f}')\n",
    "print(f'Train accuracy for h21:{train_acc[1]:.4f}, Test accuracy for h21: {test_acc[1]:.4f}')\n",
    "print(f'Train accuracy for h31:{train_acc[2]:.4f}, Test accuracy for h31: {test_acc[2]:.4f}')\n",
    "print(f'Train accuracy for h22:{train_acc[3]:.4f}, Test accuracy for h22: {test_acc[3]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f99674bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T20:05:15.870991Z",
     "iopub.status.busy": "2024-03-27T20:05:15.870673Z",
     "iopub.status.idle": "2024-03-27T20:05:15.885887Z",
     "shell.execute_reply": "2024-03-27T20:05:15.884916Z"
    },
    "papermill": {
     "duration": 0.050869,
     "end_time": "2024-03-27T20:05:15.887932",
     "exception": false,
     "start_time": "2024-03-27T20:05:15.837063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, '/kaggle/working/saved_models/CNN_GRU_hybrid_cicy4_Hodge_v3.pt')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8063104,
     "datasetId": 4575883,
     "sourceId": 7953742,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10554.762365,
   "end_time": "2024-03-27T20:05:18.516208",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-27T17:09:23.753843",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
