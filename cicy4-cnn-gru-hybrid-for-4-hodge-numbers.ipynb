{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8ec6d5",
   "metadata": {
    "papermill": {
     "duration": 0.007522,
     "end_time": "2024-03-26T06:54:52.165331",
     "exception": false,
     "start_time": "2024-03-26T06:54:52.157809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CICY4:CNN-GRU hybrid for 4 Hodge numbers\n",
    "\n",
    "- V1: Use ReduceLROnPlateau, train loss: 9.0\n",
    "- V2: Use OneCycleLR: train loss : 900\n",
    "- V3 Use CosineAnnealingLR: train loss: 60\n",
    "- V4: Use ReduceLROnPlateau again, but with higher max LR\n",
    "- V5: Change the GRU block architecture to have fewer layers, but with larger size for the hidden layer (100 -> 128), increase the size of the dense layers. Use ReduceLROnPlateau with max learning rate of 0.1, min LR of 1e-6.\n",
    "\n",
    "Best model so far is the GRU block (20, 64, 6, 4) with feat vec size of 64+384, 2 dense layers with hidden size of 512 neurons (output size is 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d612eab0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-26T06:54:52.180605Z",
     "iopub.status.busy": "2024-03-26T06:54:52.180324Z",
     "iopub.status.idle": "2024-03-26T06:54:57.707317Z",
     "shell.execute_reply": "2024-03-26T06:54:57.706527Z"
    },
    "papermill": {
     "duration": 5.537155,
     "end_time": "2024-03-26T06:54:57.709562",
     "exception": false,
     "start_time": "2024-03-26T06:54:52.172407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b321c1e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:54:57.725328Z",
     "iopub.status.busy": "2024-03-26T06:54:57.724912Z",
     "iopub.status.idle": "2024-03-26T06:55:23.933005Z",
     "shell.execute_reply": "2024-03-26T06:55:23.932121Z"
    },
    "papermill": {
     "duration": 26.218162,
     "end_time": "2024-03-26T06:55:23.935100",
     "exception": false,
     "start_time": "2024-03-26T06:54:57.716938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((921497, 16, 20), (921497, 4), (921497,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "path = '/kaggle/input/calabi-yau-cicy-4-folds/'\n",
    "\n",
    "conf = np.load('/kaggle/input/calabi-yau-cicy-4-folds/conf.npy')\n",
    "hodge = np.load(os.path.join(path, 'hodge.npy'))\n",
    "direct = np.load(os.path.join(path, 'direct.npy'))\n",
    "conf.shape, hodge.shape, direct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd78f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:23.950768Z",
     "iopub.status.busy": "2024-03-26T06:55:23.950474Z",
     "iopub.status.idle": "2024-03-26T06:55:23.954419Z",
     "shell.execute_reply": "2024-03-26T06:55:23.953600Z"
    },
    "papermill": {
     "duration": 0.014026,
     "end_time": "2024-03-26T06:55:23.956388",
     "exception": false,
     "start_time": "2024-03-26T06:55:23.942362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b79c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:23.971573Z",
     "iopub.status.busy": "2024-03-26T06:55:23.971307Z",
     "iopub.status.idle": "2024-03-26T06:55:24.101694Z",
     "shell.execute_reply": "2024-03-26T06:55:24.100957Z"
    },
    "papermill": {
     "duration": 0.140202,
     "end_time": "2024-03-26T06:55:24.103607",
     "exception": false,
     "start_time": "2024-03-26T06:55:23.963405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/calabi-yau-cicy-4-folds')\n",
    "from CICY4_functions import data_generator, batch_gd_scheduler,  calc_accuracy_mr, plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca636078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:24.118859Z",
     "iopub.status.busy": "2024-03-26T06:55:24.118573Z",
     "iopub.status.idle": "2024-03-26T06:55:24.328400Z",
     "shell.execute_reply": "2024-03-26T06:55:24.327637Z"
    },
    "papermill": {
     "duration": 0.220217,
     "end_time": "2024-03-26T06:55:24.330895",
     "exception": false,
     "start_time": "2024-03-26T06:55:24.110678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test(X, y):\n",
    "    X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101, shuffle = True)\n",
    "    \n",
    "    X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "    #only need reshape if the y dimension is 1\n",
    "    #y_train = torch.from_numpy(y_train.astype(np.float32).reshape(-1, 1))\n",
    "    y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "\n",
    "    X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "    #y_test = torch.from_numpy(y_test.astype(np.float32).reshape(-1, 1))\n",
    "    y_test = torch.from_numpy(y_test.astype(np.float32))                         \n",
    "    \n",
    "    print(f'X_train shape: {X_train.shape}, \\n y_train shape:{y_train.shape},\\\n",
    "                 \\n X_test shape: {X_test.shape}, \\n y_test shape:{y_test.shape}')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4dd091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:24.347738Z",
     "iopub.status.busy": "2024-03-26T06:55:24.347447Z",
     "iopub.status.idle": "2024-03-26T06:55:25.667920Z",
     "shell.execute_reply": "2024-03-26T06:55:25.666809Z"
    },
    "papermill": {
     "duration": 1.331814,
     "end_time": "2024-03-26T06:55:25.670172",
     "exception": false,
     "start_time": "2024-03-26T06:55:24.338358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([737197, 16, 20]), \n",
      " y_train shape:torch.Size([737197, 4]),                 \n",
      " X_test shape: torch.Size([184300, 16, 20]), \n",
      " y_test shape:torch.Size([184300, 4])\n"
     ]
    }
   ],
   "source": [
    "X = conf\n",
    "y = hodge\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test(X, y)\n",
    "\n",
    "train_gen = lambda: data_generator(X_train, y_train)\n",
    "test_gen = lambda: data_generator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831f6430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:25.685817Z",
     "iopub.status.busy": "2024-03-26T06:55:25.685520Z",
     "iopub.status.idle": "2024-03-26T06:55:25.747031Z",
     "shell.execute_reply": "2024-03-26T06:55:25.746109Z"
    },
    "papermill": {
     "duration": 0.071408,
     "end_time": "2024-03-26T06:55:25.748925",
     "exception": false,
     "start_time": "2024-03-26T06:55:25.677517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99d864",
   "metadata": {
    "papermill": {
     "duration": 0.006983,
     "end_time": "2024-03-26T06:55:25.763225",
     "exception": false,
     "start_time": "2024-03-26T06:55:25.756242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN-RNN hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3cfd67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:25.779433Z",
     "iopub.status.busy": "2024-03-26T06:55:25.778668Z",
     "iopub.status.idle": "2024-03-26T06:55:25.785652Z",
     "shell.execute_reply": "2024-03-26T06:55:25.784896Z"
    },
    "papermill": {
     "duration": 0.016878,
     "end_time": "2024-03-26T06:55:25.787576",
     "exception": false,
     "start_time": "2024-03-26T06:55:25.770698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################### CNN ###############################\n",
    "class CNN_block(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,128, 4, 1)\n",
    "        #self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128,64, 3, 1)\n",
    "        #self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.mxpool = nn.MaxPool2d(2,2)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.conv_total = nn.Sequential(\n",
    "            self.conv1,\n",
    "            #self.bn1,\n",
    "            self.mxpool,\n",
    "            #self.bn2,\n",
    "            self.conv2,\n",
    "            self.mxpool,\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv_total(x))\n",
    "        #reshape is the same as flat(x)\n",
    "        #x = x.reshape(x.shape[0], -1)\n",
    "        x = self.flat(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead75f80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:25.803154Z",
     "iopub.status.busy": "2024-03-26T06:55:25.802637Z",
     "iopub.status.idle": "2024-03-26T06:55:25.987593Z",
     "shell.execute_reply": "2024-03-26T06:55:25.986683Z"
    },
    "papermill": {
     "duration": 0.194981,
     "end_time": "2024-03-26T06:55:25.989651",
     "exception": false,
     "start_time": "2024-03-26T06:55:25.794670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_block(\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (mxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (conv_total): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_block = CNN_block()\n",
    "cnn_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2643d87e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.005765Z",
     "iopub.status.busy": "2024-03-26T06:55:26.005480Z",
     "iopub.status.idle": "2024-03-26T06:55:26.012844Z",
     "shell.execute_reply": "2024-03-26T06:55:26.012048Z"
    },
    "papermill": {
     "duration": 0.017615,
     "end_time": "2024-03-26T06:55:26.014774",
     "exception": false,
     "start_time": "2024-03-26T06:55:25.997159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN_block(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):\n",
    "        super(RNN_block,self).__init__()\n",
    "        self.D = n_inputs\n",
    "        self.M = n_hidden\n",
    "        self.K = n_outputs\n",
    "        self.L = n_rnnlayers        \n",
    "        #self.lstm = nn.LSTM(input_size = self.D,\n",
    "        #                   hidden_size = self.M,\n",
    "        #                   num_layers = self.L,\n",
    "        #                   batch_first = True)    \n",
    "        self.gru = nn.GRU(input_size = self.D,\n",
    "                           hidden_size = self.M,\n",
    "                           num_layers = self.L,\n",
    "                           batch_first = True)\n",
    "        #self.fc1 = nn.Linear(self.M, 128)\n",
    "        #self.fc2 = nn.Linear(128, self.K)\n",
    "    def forward(self, X):\n",
    "        #input X is NxTxD\n",
    "        #initial hidden states\n",
    "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        #c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        #get LSTM unit output:\n",
    "        #output is NxTxM\n",
    "        #out, _ = self.lstm(X, (h0,c0))\n",
    "        out, _ = self.gru(X, h0)   \n",
    "        #we only want h(T) at the final time step\n",
    "        # output is now of shape (N, M)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a752435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.030467Z",
     "iopub.status.busy": "2024-03-26T06:55:26.030201Z",
     "iopub.status.idle": "2024-03-26T06:55:26.159667Z",
     "shell.execute_reply": "2024-03-26T06:55:26.158753Z"
    },
    "papermill": {
     "duration": 0.139527,
     "end_time": "2024-03-26T06:55:26.161599",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.022072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_block(\n",
       "  (gru): GRU(20, 128, num_layers=4, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gru_block = RNN_block(20, 64, 6, 4)\n",
    "gru_block = RNN_block(20, 128, 4, 4)\n",
    "gru_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37e01dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.177962Z",
     "iopub.status.busy": "2024-03-26T06:55:26.177643Z",
     "iopub.status.idle": "2024-03-26T06:55:26.184716Z",
     "shell.execute_reply": "2024-03-26T06:55:26.183889Z"
    },
    "papermill": {
     "duration": 0.017451,
     "end_time": "2024-03-26T06:55:26.186635",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.169184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_RNN_hybrid(nn.Module):\n",
    "    def __init__(self, cnn_block, rnn_block, feat_vec_size):\n",
    "        super(CNN_RNN_hybrid, self).__init__()\n",
    "        self.cnn_block = cnn_block\n",
    "        self.rnn_block = rnn_block\n",
    "        self.feat_vec_size = feat_vec_size\n",
    "        self.fc1 = nn.Linear(self.feat_vec_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #output of cnn block is (N,384)\n",
    "        x1 = x.view(-1,1, 16,20)\n",
    "        x1 = self.cnn_block(x1)\n",
    "        #output of rnn block is (N,M = 64)\n",
    "        x2 = self.rnn_block(x)\n",
    "        #concatenate the 2 outputs to produce a feat vec (N, M+384)\n",
    "        xx = torch.cat([x1, x2], dim = 1)\n",
    "        # pass through linear layers\n",
    "        xx = self.fc1(xx)\n",
    "        #final output is 4\n",
    "        xx = self.fc2(xx)\n",
    "        \n",
    "        return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97d599c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.202596Z",
     "iopub.status.busy": "2024-03-26T06:55:26.202341Z",
     "iopub.status.idle": "2024-03-26T06:55:26.215768Z",
     "shell.execute_reply": "2024-03-26T06:55:26.214906Z"
    },
    "papermill": {
     "duration": 0.023507,
     "end_time": "2024-03-26T06:55:26.217581",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.194074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_RNN_hybrid(\n",
       "  (cnn_block): CNN_block(\n",
       "    (conv1): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (mxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "    (conv_total): Sequential(\n",
       "      (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (rnn_block): RNN_block(\n",
       "    (gru): GRU(20, 128, num_layers=4, batch_first=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_RNN_hybrid(cnn_block, gru_block, 128+384)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300969ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.233766Z",
     "iopub.status.busy": "2024-03-26T06:55:26.233512Z",
     "iopub.status.idle": "2024-03-26T06:55:26.238268Z",
     "shell.execute_reply": "2024-03-26T06:55:26.237498Z"
    },
    "papermill": {
     "duration": 0.015155,
     "end_time": "2024-03-26T06:55:26.240323",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.225168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960196\n"
     ]
    }
   ],
   "source": [
    "#count the number of parameters in the model\n",
    "params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "print(sum(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f5cad5",
   "metadata": {
    "papermill": {
     "duration": 0.009294,
     "end_time": "2024-03-26T06:55:26.258128",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.248834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Shape Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5f3c061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.274757Z",
     "iopub.status.busy": "2024-03-26T06:55:26.274500Z",
     "iopub.status.idle": "2024-03-26T06:55:26.389654Z",
     "shell.execute_reply": "2024-03-26T06:55:26.388678Z"
    },
    "papermill": {
     "duration": 0.126077,
     "end_time": "2024-03-26T06:55:26.392077",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.266000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 16, 20])\n",
      "\n",
      "Shape of the image after passing through the GRU(20, 128, num_layers=4, batch_first=True): \n",
      " torch.Size([1, 16, 128])\n",
      "\n",
      "The final output shape is torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "y = y_train[0].to(device)\n",
    "print('RNN BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "h0 = torch.zeros(gru_block.L, X.size(0), gru_block.M).to(device)\n",
    "#c0 = torch.zeros(gru_block.L, X.size(0), gru_block.M).to(device)\n",
    "X, _ = gru_block.gru(X, h0)\n",
    "print(f'Shape of the image after passing through the {gru_block.gru}: \\n {X.shape}\\n')\n",
    "\n",
    "#get only the h(T) at the last time step\n",
    "Xg = X[:, -1, :]\n",
    "print(f'The final output shape is {Xg.shape}')\n",
    "#print(X)\n",
    "#print(f'Target: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af33a7c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.413430Z",
     "iopub.status.busy": "2024-03-26T06:55:26.413093Z",
     "iopub.status.idle": "2024-03-26T06:55:26.816340Z",
     "shell.execute_reply": "2024-03-26T06:55:26.815003Z"
    },
    "papermill": {
     "duration": 0.415956,
     "end_time": "2024-03-26T06:55:26.818394",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.402438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 1, 16, 20])\n",
      "\n",
      "Original shape of the image after passing through Sequential(\n",
      "  (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "): \n",
      " torch.Size([1, 64, 2, 3])\n",
      "\n",
      "Original shape of the image after passing through Flatten(start_dim=1, end_dim=-1): \n",
      " torch.Size([1, 384])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "print('CNN BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "X = cnn_block.conv_total(X)\n",
    "print(f'Original shape of the image after passing through {cnn_block.conv_total}: \\n {X.shape}\\n')\n",
    "\n",
    "#X = X.reshape(X.shape[0], -1)\n",
    "#print(f'After reshaping: {X.shape}') [1,384]\n",
    "Xc = cnn_block.flat(X)\n",
    "print(f'Original shape of the image after passing through {cnn_block.flat}: \\n {Xc.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39078660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.835970Z",
     "iopub.status.busy": "2024-03-26T06:55:26.835632Z",
     "iopub.status.idle": "2024-03-26T06:55:26.860181Z",
     "shell.execute_reply": "2024-03-26T06:55:26.859270Z"
    },
    "papermill": {
     "duration": 0.035558,
     "end_time": "2024-03-26T06:55:26.862183",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.826625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([1, 384]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat = torch.cat([Xc, Xg], dim=1) \n",
    "Xg.shape, Xc.shape, X_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f43b9bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.879124Z",
     "iopub.status.busy": "2024-03-26T06:55:26.878825Z",
     "iopub.status.idle": "2024-03-26T06:55:26.938475Z",
     "shell.execute_reply": "2024-03-26T06:55:26.937260Z"
    },
    "papermill": {
     "duration": 0.070369,
     "end_time": "2024-03-26T06:55:26.940523",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.870154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-RNN HYBRID BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 16, 20])\n",
      "\n",
      "Shape  after passing through the entire network: \n",
      " torch.Size([1, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "print('CNN-RNN HYBRID BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "X = model(X)\n",
    "print(f'Shape  after passing through the entire network: \\n {X.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6cbdd3",
   "metadata": {
    "papermill": {
     "duration": 0.007935,
     "end_time": "2024-03-26T06:55:26.956585",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.948650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train loop with scheduler\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html\n",
    "\n",
    "https://residentmario.github.io/pytorch-training-performance-guide/lr-sched-and-optim.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40be6ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:26.974303Z",
     "iopub.status.busy": "2024-03-26T06:55:26.974030Z",
     "iopub.status.idle": "2024-03-26T06:55:29.335063Z",
     "shell.execute_reply": "2024-03-26T06:55:29.334300Z"
    },
    "papermill": {
     "duration": 2.372237,
     "end_time": "2024-03-26T06:55:29.337265",
     "exception": false,
     "start_time": "2024-03-26T06:55:26.965028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epoch = 200\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 128\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.01)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', min_lr = 1e-6)\n",
    "#scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "#scheduler = lr_scheduler.OneCycleLR(optimizer=optimizer, epochs=max_epoch,\n",
    "#            pct_start=0.0, steps_per_epoch=len(X_train)//batch_size,\n",
    "#            max_lr= 0.01, div_factor=25, final_div_factor=4.0e-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e71bc9db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T06:55:29.355918Z",
     "iopub.status.busy": "2024-03-26T06:55:29.355477Z",
     "iopub.status.idle": "2024-03-26T08:48:06.571386Z",
     "shell.execute_reply": "2024-03-26T08:48:06.570387Z"
    },
    "papermill": {
     "duration": 6757.251159,
     "end_time": "2024-03-26T08:48:06.597033",
     "exception": false,
     "start_time": "2024-03-26T06:55:29.345874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, train loss:  342.0023          test_loss:  201.8847, duration: 0:00:34.372422,           learning rate: (0.01, 0.01)\n",
      "Epoch: 2/200, train loss:  218.3929          test_loss:  215.3044, duration: 0:00:34.007090,           learning rate: (0.01, 0.01)\n",
      "Epoch: 3/200, train loss:  198.4181          test_loss:  204.5224, duration: 0:00:33.893166,           learning rate: (0.01, 0.01)\n",
      "Epoch: 4/200, train loss:  190.0422          test_loss:  197.9765, duration: 0:00:34.078006,           learning rate: (0.01, 0.01)\n",
      "Epoch: 5/200, train loss:  188.5250          test_loss:  179.6162, duration: 0:00:34.086329,           learning rate: (0.01, 0.01)\n",
      "Epoch: 6/200, train loss:  185.7047          test_loss:  195.0037, duration: 0:00:34.015143,           learning rate: (0.01, 0.01)\n",
      "Epoch: 7/200, train loss:  182.7977          test_loss:  180.2291, duration: 0:00:33.892951,           learning rate: (0.01, 0.01)\n",
      "Epoch: 8/200, train loss:  183.0391          test_loss:  178.3705, duration: 0:00:33.883324,           learning rate: (0.01, 0.01)\n",
      "Epoch: 9/200, train loss:  182.3024          test_loss:  181.1946, duration: 0:00:34.015885,           learning rate: (0.01, 0.01)\n",
      "Epoch: 10/200, train loss:  179.4457          test_loss:  166.1512, duration: 0:00:33.934327,           learning rate: (0.01, 0.01)\n",
      "Epoch: 11/200, train loss:  180.3640          test_loss:  190.5784, duration: 0:00:33.909813,           learning rate: (0.01, 0.01)\n",
      "Epoch: 12/200, train loss:  179.8187          test_loss:  186.4683, duration: 0:00:33.930177,           learning rate: (0.01, 0.01)\n",
      "Epoch: 13/200, train loss:  178.7905          test_loss:  170.7516, duration: 0:00:33.918445,           learning rate: (0.01, 0.01)\n",
      "Epoch: 14/200, train loss:  193.1656          test_loss:  174.7285, duration: 0:00:33.990300,           learning rate: (0.01, 0.01)\n",
      "Epoch: 15/200, train loss:  175.6035          test_loss:  174.1147, duration: 0:00:33.867904,           learning rate: (0.01, 0.01)\n",
      "Epoch: 16/200, train loss:  175.0043          test_loss:  180.6573, duration: 0:00:34.219559,           learning rate: (0.01, 0.01)\n",
      "Epoch: 17/200, train loss:  173.5016          test_loss:  181.4269, duration: 0:00:34.352084,           learning rate: (0.01, 0.01)\n",
      "Epoch: 18/200, train loss:  361.8140          test_loss:  160.8097, duration: 0:00:34.308822,           learning rate: (0.01, 0.01)\n",
      "Epoch: 19/200, train loss:  174.2796          test_loss:  267.8802, duration: 0:00:34.286307,           learning rate: (0.01, 0.01)\n",
      "Epoch: 20/200, train loss:  191.5173          test_loss:  165.6329, duration: 0:00:34.331642,           learning rate: (0.01, 0.01)\n",
      "Epoch: 21/200, train loss:  166.4866          test_loss:  190.4461, duration: 0:00:34.413495,           learning rate: (0.01, 0.01)\n",
      "Epoch: 22/200, train loss:  163.7584          test_loss:  161.4676, duration: 0:00:34.345250,           learning rate: (0.01, 0.01)\n",
      "Epoch: 23/200, train loss:  162.8367          test_loss:  149.1370, duration: 0:00:34.270180,           learning rate: (0.01, 0.01)\n",
      "Epoch: 24/200, train loss:  161.7811          test_loss:  154.8140, duration: 0:00:34.226876,           learning rate: (0.01, 0.01)\n",
      "Epoch: 25/200, train loss:  159.0048          test_loss:  159.4998, duration: 0:00:34.127818,           learning rate: (0.01, 0.01)\n",
      "Epoch: 26/200, train loss:  158.5425          test_loss:  214.9822, duration: 0:00:34.177906,           learning rate: (0.01, 0.01)\n",
      "Epoch: 27/200, train loss:  159.5515          test_loss:  165.1199, duration: 0:00:34.193208,           learning rate: (0.01, 0.01)\n",
      "Epoch: 28/200, train loss:  157.2139          test_loss:  155.9568, duration: 0:00:34.104526,           learning rate: (0.01, 0.01)\n",
      "Epoch: 29/200, train loss:  157.3530          test_loss:  156.5042, duration: 0:00:34.049715,           learning rate: (0.01, 0.01)\n",
      "Epoch: 30/200, train loss:  157.7124          test_loss:  158.6462, duration: 0:00:34.119174,           learning rate: (0.01, 0.01)\n",
      "Epoch: 31/200, train loss:  175.1483          test_loss:  154.3185, duration: 0:00:34.153125,           learning rate: (0.01, 0.01)\n",
      "Epoch: 32/200, train loss:  156.8017          test_loss:  179.9568, duration: 0:00:34.292763,           learning rate: (0.01, 0.01)\n",
      "Epoch: 33/200, train loss:  157.2311          test_loss:  167.4585, duration: 0:00:34.171885,           learning rate: (0.01, 0.01)\n",
      "Epoch: 34/200, train loss:  157.4299          test_loss:  151.3387, duration: 0:00:34.197585,           learning rate: (0.01, 0.001)\n",
      "Epoch: 35/200, train loss:  113.1161          test_loss:  114.4720, duration: 0:00:34.126400,           learning rate: (0.001, 0.001)\n",
      "Epoch: 36/200, train loss:  105.0416          test_loss:  108.8711, duration: 0:00:34.133630,           learning rate: (0.001, 0.001)\n",
      "Epoch: 37/200, train loss:  101.8135          test_loss:  108.2750, duration: 0:00:34.094192,           learning rate: (0.001, 0.001)\n",
      "Epoch: 38/200, train loss:  99.5413          test_loss:  107.5289, duration: 0:00:34.093039,           learning rate: (0.001, 0.001)\n",
      "Epoch: 39/200, train loss:  97.7302          test_loss:  106.2270, duration: 0:00:33.995903,           learning rate: (0.001, 0.001)\n",
      "Epoch: 40/200, train loss:  96.7824          test_loss:  104.8421, duration: 0:00:34.008476,           learning rate: (0.001, 0.001)\n",
      "Epoch: 41/200, train loss:  95.6579          test_loss:  103.2605, duration: 0:00:34.097833,           learning rate: (0.001, 0.001)\n",
      "Epoch: 42/200, train loss:  95.0357          test_loss:  101.8169, duration: 0:00:34.120328,           learning rate: (0.001, 0.001)\n",
      "Epoch: 43/200, train loss:  94.3219          test_loss:  103.3453, duration: 0:00:33.950461,           learning rate: (0.001, 0.001)\n",
      "Epoch: 44/200, train loss:  94.2133          test_loss:  104.9337, duration: 0:00:34.044949,           learning rate: (0.001, 0.001)\n",
      "Epoch: 45/200, train loss:  93.8155          test_loss:  112.8935, duration: 0:00:34.015511,           learning rate: (0.001, 0.001)\n",
      "Epoch: 46/200, train loss:  94.2193          test_loss:  102.3230, duration: 0:00:33.660668,           learning rate: (0.001, 0.001)\n",
      "Epoch: 47/200, train loss:  93.8689          test_loss:  102.0205, duration: 0:00:33.627286,           learning rate: (0.001, 0.001)\n",
      "Epoch: 48/200, train loss:  93.6840          test_loss:  101.4309, duration: 0:00:33.604316,           learning rate: (0.001, 0.001)\n",
      "Epoch: 49/200, train loss:  93.2816          test_loss:  103.3478, duration: 0:00:33.697035,           learning rate: (0.001, 0.001)\n",
      "Epoch: 50/200, train loss:  93.3031          test_loss:  98.3530, duration: 0:00:33.694133,           learning rate: (0.001, 0.001)\n",
      "Epoch: 51/200, train loss:  92.8359          test_loss:  103.3511, duration: 0:00:33.698676,           learning rate: (0.001, 0.001)\n",
      "Epoch: 52/200, train loss:  90.8068          test_loss:  97.8134, duration: 0:00:33.669073,           learning rate: (0.001, 0.001)\n",
      "Epoch: 53/200, train loss:  87.0861          test_loss:  92.9171, duration: 0:00:33.791844,           learning rate: (0.001, 0.001)\n",
      "Epoch: 54/200, train loss:  83.7663          test_loss:  92.2384, duration: 0:00:33.722930,           learning rate: (0.001, 0.001)\n",
      "Epoch: 55/200, train loss:  82.2230          test_loss:  88.1972, duration: 0:00:33.649870,           learning rate: (0.001, 0.001)\n",
      "Epoch: 56/200, train loss:  79.4026          test_loss:  83.8090, duration: 0:00:33.649420,           learning rate: (0.001, 0.001)\n",
      "Epoch: 57/200, train loss:  77.8042          test_loss:  81.8721, duration: 0:00:33.593244,           learning rate: (0.001, 0.001)\n",
      "Epoch: 58/200, train loss:  73.8825          test_loss:  78.9302, duration: 0:00:33.720252,           learning rate: (0.001, 0.001)\n",
      "Epoch: 59/200, train loss:  70.5153          test_loss:  78.5759, duration: 0:00:33.623185,           learning rate: (0.001, 0.001)\n",
      "Epoch: 60/200, train loss:  68.1841          test_loss:  95.5989, duration: 0:00:33.614041,           learning rate: (0.001, 0.001)\n",
      "Epoch: 61/200, train loss:  65.8981          test_loss:  70.6448, duration: 0:00:33.637198,           learning rate: (0.001, 0.001)\n",
      "Epoch: 62/200, train loss:  64.6465          test_loss:  68.1791, duration: 0:00:33.738554,           learning rate: (0.001, 0.001)\n",
      "Epoch: 63/200, train loss:  61.6014          test_loss:  64.7876, duration: 0:00:33.573597,           learning rate: (0.001, 0.001)\n",
      "Epoch: 64/200, train loss:  59.4466          test_loss:  60.8613, duration: 0:00:33.618432,           learning rate: (0.001, 0.001)\n",
      "Epoch: 65/200, train loss:  57.8984          test_loss:  60.4088, duration: 0:00:33.690080,           learning rate: (0.001, 0.001)\n",
      "Epoch: 66/200, train loss:  55.0615          test_loss:  55.4370, duration: 0:00:33.669284,           learning rate: (0.001, 0.001)\n",
      "Epoch: 67/200, train loss:  53.1273          test_loss:  55.7913, duration: 0:00:33.656062,           learning rate: (0.001, 0.001)\n",
      "Epoch: 68/200, train loss:  50.4529          test_loss:  55.3502, duration: 0:00:33.849212,           learning rate: (0.001, 0.001)\n",
      "Epoch: 69/200, train loss:  49.0305          test_loss:  49.0314, duration: 0:00:33.633572,           learning rate: (0.001, 0.001)\n",
      "Epoch: 70/200, train loss:  46.4805          test_loss:  46.0718, duration: 0:00:33.682354,           learning rate: (0.001, 0.001)\n",
      "Epoch: 71/200, train loss:  44.2485          test_loss:  52.0553, duration: 0:00:33.629990,           learning rate: (0.001, 0.001)\n",
      "Epoch: 72/200, train loss:  40.8872          test_loss:  42.1792, duration: 0:00:33.766295,           learning rate: (0.001, 0.001)\n",
      "Epoch: 73/200, train loss:  39.4083          test_loss:  38.2058, duration: 0:00:33.678998,           learning rate: (0.001, 0.001)\n",
      "Epoch: 74/200, train loss:  37.4599          test_loss:  34.4638, duration: 0:00:33.647473,           learning rate: (0.001, 0.001)\n",
      "Epoch: 75/200, train loss:  31.5583          test_loss:  33.9788, duration: 0:00:33.673200,           learning rate: (0.001, 0.001)\n",
      "Epoch: 76/200, train loss:  30.0560          test_loss:  32.3315, duration: 0:00:33.665357,           learning rate: (0.001, 0.001)\n",
      "Epoch: 77/200, train loss:  28.1518          test_loss:  31.2094, duration: 0:00:33.603981,           learning rate: (0.001, 0.001)\n",
      "Epoch: 78/200, train loss:  26.4592          test_loss:  27.5331, duration: 0:00:33.636856,           learning rate: (0.001, 0.001)\n",
      "Epoch: 79/200, train loss:  25.9676          test_loss:  24.6607, duration: 0:00:33.634114,           learning rate: (0.001, 0.001)\n",
      "Epoch: 80/200, train loss:  24.0079          test_loss:  23.4311, duration: 0:00:33.577276,           learning rate: (0.001, 0.001)\n",
      "Epoch: 81/200, train loss:  23.2314          test_loss:  36.6209, duration: 0:00:33.603830,           learning rate: (0.001, 0.001)\n",
      "Epoch: 82/200, train loss:  23.2334          test_loss:  25.5623, duration: 0:00:33.738041,           learning rate: (0.001, 0.001)\n",
      "Epoch: 83/200, train loss:  21.7310          test_loss:  22.4473, duration: 0:00:33.645061,           learning rate: (0.001, 0.001)\n",
      "Epoch: 84/200, train loss:  21.4309          test_loss:  20.5028, duration: 0:00:33.694043,           learning rate: (0.001, 0.001)\n",
      "Epoch: 85/200, train loss:  20.8979          test_loss:  22.7978, duration: 0:00:33.671064,           learning rate: (0.001, 0.001)\n",
      "Epoch: 86/200, train loss:  19.9722          test_loss:  23.7882, duration: 0:00:33.680910,           learning rate: (0.001, 0.001)\n",
      "Epoch: 87/200, train loss:  20.0317          test_loss:  28.0234, duration: 0:00:33.758701,           learning rate: (0.001, 0.001)\n",
      "Epoch: 88/200, train loss:  19.1651          test_loss:  27.7296, duration: 0:00:34.037522,           learning rate: (0.001, 0.001)\n",
      "Epoch: 89/200, train loss:  19.2399          test_loss:  19.1149, duration: 0:00:33.841577,           learning rate: (0.001, 0.001)\n",
      "Epoch: 90/200, train loss:  19.3115          test_loss:  24.3803, duration: 0:00:33.805236,           learning rate: (0.001, 0.001)\n",
      "Epoch: 91/200, train loss:  18.9468          test_loss:  18.9010, duration: 0:00:33.675474,           learning rate: (0.001, 0.001)\n",
      "Epoch: 92/200, train loss:  18.1274          test_loss:  29.2600, duration: 0:00:33.632106,           learning rate: (0.001, 0.001)\n",
      "Epoch: 93/200, train loss:  18.0161          test_loss:  31.5837, duration: 0:00:33.690580,           learning rate: (0.001, 0.001)\n",
      "Epoch: 94/200, train loss:  18.3369          test_loss:  16.9572, duration: 0:00:33.745098,           learning rate: (0.001, 0.001)\n",
      "Epoch: 95/200, train loss:  18.8687          test_loss:  18.3316, duration: 0:00:33.702519,           learning rate: (0.001, 0.001)\n",
      "Epoch: 96/200, train loss:  16.3665          test_loss:  17.3506, duration: 0:00:33.660205,           learning rate: (0.001, 0.001)\n",
      "Epoch: 97/200, train loss:  17.5608          test_loss:  19.6610, duration: 0:00:33.629583,           learning rate: (0.001, 0.001)\n",
      "Epoch: 98/200, train loss:  16.5536          test_loss:  61.0573, duration: 0:00:33.752015,           learning rate: (0.001, 0.001)\n",
      "Epoch: 99/200, train loss:  16.6904          test_loss:  18.3522, duration: 0:00:33.717672,           learning rate: (0.001, 0.001)\n",
      "Epoch: 100/200, train loss:  16.5889          test_loss:  17.2934, duration: 0:00:33.746958,           learning rate: (0.001, 0.001)\n",
      "Epoch: 101/200, train loss:  15.8809          test_loss:  34.6012, duration: 0:00:33.708317,           learning rate: (0.001, 0.001)\n",
      "Epoch: 102/200, train loss:  15.4280          test_loss:  15.9907, duration: 0:00:34.007782,           learning rate: (0.001, 0.001)\n",
      "Epoch: 103/200, train loss:  15.7684          test_loss:  15.2068, duration: 0:00:33.659633,           learning rate: (0.001, 0.001)\n",
      "Epoch: 104/200, train loss:  16.6450          test_loss:  14.4177, duration: 0:00:33.714609,           learning rate: (0.001, 0.001)\n",
      "Epoch: 105/200, train loss:  15.9023          test_loss:  14.7575, duration: 0:00:33.656247,           learning rate: (0.001, 0.001)\n",
      "Epoch: 106/200, train loss:  14.3339          test_loss:  16.7172, duration: 0:00:33.702536,           learning rate: (0.001, 0.001)\n",
      "Epoch: 107/200, train loss:  17.5407          test_loss:  21.1080, duration: 0:00:33.675547,           learning rate: (0.001, 0.001)\n",
      "Epoch: 108/200, train loss:  15.5705          test_loss:  16.8539, duration: 0:00:33.611881,           learning rate: (0.001, 0.001)\n",
      "Epoch: 109/200, train loss:  14.2267          test_loss:  20.5758, duration: 0:00:33.654803,           learning rate: (0.001, 0.001)\n",
      "Epoch: 110/200, train loss:  15.1940          test_loss:  15.2898, duration: 0:00:33.727184,           learning rate: (0.001, 0.001)\n",
      "Epoch: 111/200, train loss:  14.3638          test_loss:  16.3070, duration: 0:00:33.708724,           learning rate: (0.001, 0.001)\n",
      "Epoch: 112/200, train loss:  14.2494          test_loss:  13.8371, duration: 0:00:33.619627,           learning rate: (0.001, 0.001)\n",
      "Epoch: 113/200, train loss:  14.8774          test_loss:  16.4733, duration: 0:00:33.635587,           learning rate: (0.001, 0.001)\n",
      "Epoch: 114/200, train loss:  14.6695          test_loss:  23.2140, duration: 0:00:33.711344,           learning rate: (0.001, 0.001)\n",
      "Epoch: 115/200, train loss:  15.2516          test_loss:  15.4181, duration: 0:00:33.630814,           learning rate: (0.001, 0.001)\n",
      "Epoch: 116/200, train loss:  14.2315          test_loss:  13.1332, duration: 0:00:33.713538,           learning rate: (0.001, 0.001)\n",
      "Epoch: 117/200, train loss:  15.3352          test_loss:  14.6583, duration: 0:00:33.707207,           learning rate: (0.001, 0.001)\n",
      "Epoch: 118/200, train loss:  14.8380          test_loss:  13.8196, duration: 0:00:33.692834,           learning rate: (0.001, 0.001)\n",
      "Epoch: 119/200, train loss:  14.9474          test_loss:  13.3580, duration: 0:00:33.646962,           learning rate: (0.001, 0.001)\n",
      "Epoch: 120/200, train loss:  15.3853          test_loss:  14.0162, duration: 0:00:33.736591,           learning rate: (0.001, 0.001)\n",
      "Epoch: 121/200, train loss:  15.8054          test_loss:  13.0407, duration: 0:00:33.757435,           learning rate: (0.001, 0.001)\n",
      "Epoch: 122/200, train loss:  13.2952          test_loss:  13.6000, duration: 0:00:33.723758,           learning rate: (0.001, 0.001)\n",
      "Epoch: 123/200, train loss:  13.9658          test_loss:  13.7089, duration: 0:00:33.648810,           learning rate: (0.001, 0.001)\n",
      "Epoch: 124/200, train loss:  14.8502          test_loss:  13.6175, duration: 0:00:33.662728,           learning rate: (0.001, 0.001)\n",
      "Epoch: 125/200, train loss:  13.3322          test_loss:  14.5362, duration: 0:00:33.702469,           learning rate: (0.001, 0.001)\n",
      "Epoch: 126/200, train loss:  14.2783          test_loss:  14.5109, duration: 0:00:33.680684,           learning rate: (0.001, 0.001)\n",
      "Epoch: 127/200, train loss:  13.1025          test_loss:  13.6474, duration: 0:00:33.648756,           learning rate: (0.001, 0.001)\n",
      "Epoch: 128/200, train loss:  12.8634          test_loss:  17.7368, duration: 0:00:33.655599,           learning rate: (0.001, 0.001)\n",
      "Epoch: 129/200, train loss:  12.6881          test_loss:  16.8951, duration: 0:00:33.830521,           learning rate: (0.001, 0.001)\n",
      "Epoch: 130/200, train loss:  13.3394          test_loss:  14.7960, duration: 0:00:33.726189,           learning rate: (0.001, 0.001)\n",
      "Epoch: 131/200, train loss:  13.0084          test_loss:  15.6458, duration: 0:00:33.634354,           learning rate: (0.001, 0.001)\n",
      "Epoch: 132/200, train loss:  14.1777          test_loss:  14.4027, duration: 0:00:33.688565,           learning rate: (0.001, 0.0001)\n",
      "Epoch: 133/200, train loss:  7.0791          test_loss:  9.0299, duration: 0:00:33.667776,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 134/200, train loss:  6.3426          test_loss:  8.4617, duration: 0:00:33.635361,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 135/200, train loss:  6.0512          test_loss:  8.2281, duration: 0:00:33.624484,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 136/200, train loss:  5.8707          test_loss:  8.4325, duration: 0:00:33.645515,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 137/200, train loss:  5.7301          test_loss:  7.9967, duration: 0:00:33.676627,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 138/200, train loss:  5.6158          test_loss:  8.0810, duration: 0:00:33.646430,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 139/200, train loss:  5.5204          test_loss:  7.8748, duration: 0:00:33.625329,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 140/200, train loss:  5.4321          test_loss:  7.8841, duration: 0:00:33.547262,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 141/200, train loss:  5.3467          test_loss:  7.8514, duration: 0:00:33.650759,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 142/200, train loss:  5.2790          test_loss:  7.6749, duration: 0:00:33.714111,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 143/200, train loss:  5.2134          test_loss:  7.5427, duration: 0:00:33.802103,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 144/200, train loss:  5.1579          test_loss:  7.5056, duration: 0:00:33.649716,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 145/200, train loss:  5.0993          test_loss:  7.4341, duration: 0:00:33.675164,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 146/200, train loss:  5.0472          test_loss:  7.4191, duration: 0:00:33.954862,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 147/200, train loss:  5.0053          test_loss:  7.5200, duration: 0:00:33.786729,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 148/200, train loss:  4.9555          test_loss:  7.3200, duration: 0:00:33.727320,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 149/200, train loss:  4.9077          test_loss:  7.2461, duration: 0:00:33.774259,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 150/200, train loss:  4.8685          test_loss:  7.1778, duration: 0:00:33.659787,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 151/200, train loss:  4.8246          test_loss:  7.2701, duration: 0:00:33.666495,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 152/200, train loss:  4.7842          test_loss:  7.1656, duration: 0:00:33.575491,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 153/200, train loss:  4.7489          test_loss:  7.1755, duration: 0:00:33.640360,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 154/200, train loss:  4.7114          test_loss:  7.0873, duration: 0:00:33.738312,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 155/200, train loss:  4.6758          test_loss:  7.1427, duration: 0:00:33.665737,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 156/200, train loss:  4.6452          test_loss:  7.2031, duration: 0:00:33.715483,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 157/200, train loss:  4.6088          test_loss:  7.0079, duration: 0:00:33.790370,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 158/200, train loss:  4.5804          test_loss:  7.1403, duration: 0:00:33.639388,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 159/200, train loss:  4.5519          test_loss:  7.0437, duration: 0:00:33.624063,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 160/200, train loss:  4.5164          test_loss:  6.9577, duration: 0:00:33.580878,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 161/200, train loss:  4.4864          test_loss:  6.9148, duration: 0:00:33.652272,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 162/200, train loss:  4.4622          test_loss:  7.0094, duration: 0:00:33.593198,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 163/200, train loss:  4.4305          test_loss:  6.9486, duration: 0:00:33.630112,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 164/200, train loss:  4.4127          test_loss:  6.8724, duration: 0:00:33.552281,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 165/200, train loss:  4.3837          test_loss:  6.8006, duration: 0:00:33.890814,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 166/200, train loss:  4.3536          test_loss:  6.8452, duration: 0:00:34.208814,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 167/200, train loss:  4.3304          test_loss:  6.7784, duration: 0:00:33.775747,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 168/200, train loss:  4.3065          test_loss:  6.9724, duration: 0:00:33.752784,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 169/200, train loss:  4.2906          test_loss:  6.8898, duration: 0:00:33.789367,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 170/200, train loss:  4.2574          test_loss:  6.8735, duration: 0:00:33.716356,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 171/200, train loss:  4.2360          test_loss:  6.8680, duration: 0:00:33.708392,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 172/200, train loss:  4.2201          test_loss:  6.8855, duration: 0:00:33.651646,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 173/200, train loss:  4.2003          test_loss:  6.7505, duration: 0:00:33.686154,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 174/200, train loss:  4.1736          test_loss:  6.9708, duration: 0:00:33.667796,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 175/200, train loss:  4.1586          test_loss:  6.6198, duration: 0:00:33.933010,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 176/200, train loss:  4.1343          test_loss:  6.6525, duration: 0:00:33.707248,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 177/200, train loss:  4.1167          test_loss:  6.7892, duration: 0:00:33.852255,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 178/200, train loss:  4.0975          test_loss:  6.5490, duration: 0:00:33.689725,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 179/200, train loss:  4.0797          test_loss:  6.6962, duration: 0:00:33.649074,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 180/200, train loss:  4.0605          test_loss:  6.9027, duration: 0:00:33.607431,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 181/200, train loss:  4.0508          test_loss:  6.4941, duration: 0:00:33.641937,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 182/200, train loss:  4.0262          test_loss:  6.6006, duration: 0:00:33.590057,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 183/200, train loss:  4.0081          test_loss:  6.8982, duration: 0:00:33.556271,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 184/200, train loss:  3.9977          test_loss:  6.5269, duration: 0:00:33.651102,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 185/200, train loss:  3.9785          test_loss:  6.8049, duration: 0:00:33.804389,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 186/200, train loss:  3.9572          test_loss:  6.6431, duration: 0:00:33.720055,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 187/200, train loss:  3.9435          test_loss:  6.5377, duration: 0:00:33.605110,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 188/200, train loss:  3.9236          test_loss:  6.5058, duration: 0:00:33.577650,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 189/200, train loss:  3.9111          test_loss:  6.4342, duration: 0:00:33.672998,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 190/200, train loss:  3.8948          test_loss:  6.5224, duration: 0:00:33.727668,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 191/200, train loss:  3.8820          test_loss:  6.3579, duration: 0:00:33.632181,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 192/200, train loss:  3.8737          test_loss:  6.4169, duration: 0:00:33.606454,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 193/200, train loss:  3.8548          test_loss:  6.3514, duration: 0:00:33.624313,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 194/200, train loss:  3.8388          test_loss:  6.5210, duration: 0:00:33.780290,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 195/200, train loss:  3.8256          test_loss:  6.3449, duration: 0:00:33.828022,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 196/200, train loss:  3.8133          test_loss:  6.2658, duration: 0:00:33.670847,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 197/200, train loss:  3.7943          test_loss:  6.3916, duration: 0:00:33.728464,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 198/200, train loss:  3.7850          test_loss:  6.4054, duration: 0:00:33.654160,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 199/200, train loss:  3.7807          test_loss:  6.2607, duration: 0:00:33.924479,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 200/200, train loss:  3.7626          test_loss:  6.1883, duration: 0:00:33.570648,           learning rate: (0.0001, 0.0001)\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses=batch_gd_scheduler(model, criterion, optimizer, train_gen, test_gen, scheduler, \n",
    "                                             max_epoch, device,  cnn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87dc6028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T08:48:06.644700Z",
     "iopub.status.busy": "2024-03-26T08:48:06.644398Z",
     "iopub.status.idle": "2024-03-26T08:48:23.303561Z",
     "shell.execute_reply": "2024-03-26T08:48:23.302619Z"
    },
    "papermill": {
     "duration": 16.685185,
     "end_time": "2024-03-26T08:48:23.305541",
     "exception": false,
     "start_time": "2024-03-26T08:48:06.620356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for h11:0.8142, Test accuracy for h11: 0.8117\n",
      "Train accuracy for h21:0.6988, Test accuracy for h21: 0.6944\n",
      "Train accuracy for h31:0.4687, Test accuracy for h31: 0.4351\n",
      "Train accuracy for h22:0.1479, Test accuracy for h22: 0.1340\n"
     ]
    }
   ],
   "source": [
    "train_acc, test_acc = calc_accuracy_mr(model, train_gen , test_gen, device = device, cnn= False)\n",
    "print(f'Train accuracy for h11:{train_acc[0]:.4f}, Test accuracy for h11: {test_acc[0]:.4f}')\n",
    "print(f'Train accuracy for h21:{train_acc[1]:.4f}, Test accuracy for h21: {test_acc[1]:.4f}')\n",
    "print(f'Train accuracy for h31:{train_acc[2]:.4f}, Test accuracy for h31: {test_acc[2]:.4f}')\n",
    "print(f'Train accuracy for h22:{train_acc[3]:.4f}, Test accuracy for h22: {test_acc[3]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b94c41c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T08:48:23.353996Z",
     "iopub.status.busy": "2024-03-26T08:48:23.353232Z",
     "iopub.status.idle": "2024-03-26T08:48:23.368004Z",
     "shell.execute_reply": "2024-03-26T08:48:23.367299Z"
    },
    "papermill": {
     "duration": 0.040531,
     "end_time": "2024-03-26T08:48:23.369785",
     "exception": false,
     "start_time": "2024-03-26T08:48:23.329254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, '/kaggle/working/saved_models/CNN_GRU_hybrid_cicy4_Hodge_v1.pt')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4575883,
     "sourceId": 7936105,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6815.700807,
   "end_time": "2024-03-26T08:48:25.129095",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-26T06:54:49.428288",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
