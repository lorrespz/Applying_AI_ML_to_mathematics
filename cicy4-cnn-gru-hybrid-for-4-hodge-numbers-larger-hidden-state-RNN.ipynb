{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079dcbbb",
   "metadata": {
    "papermill": {
     "duration": 0.007744,
     "end_time": "2024-03-26T09:27:43.134817",
     "exception": false,
     "start_time": "2024-03-26T09:27:43.127073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CICY4:CNN-GRU hybrid for 4 Hodge numbers\n",
    "\n",
    "- V1: Use ReduceLROnPlateau with GRU block (20, 64, 6, 4), train loss ~ 9.0\n",
    "- V2: Use OneCycleLR: train loss ~ 900 (bad result)\n",
    "- V3 Use CosineAnnealingLR: train loss ~ 60 (bad result)\n",
    "- V4: Use ReduceLROnPlateau again, but with higher max LR (bad result)\n",
    "- V5/V6: Change the GRU block architecture to have fewer layers, but with larger size for the hidden layer (100 -> 128), increase the size of the dense layers. Use ReduceLROnPlateau with max learning rate of 0.1, min LR of 1e-6. Very good result obtained: (81, 69, 48, 13) accuracy for (h11, h21, h31, h22). Train loss ~ 3.0, test loss ~ 6.0\n",
    "- V7: Increase the hidden size of the GRU block from 128->256 and reduce the num_layer from 4-> 2. Same learning parameters as V5. \n",
    "\n",
    "For GRU block, the standalone best model so far has the parameters (in_channels, hidden_size, num_layers, outputs) =  (20, 64, 6, 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b567a5d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-26T09:27:43.151591Z",
     "iopub.status.busy": "2024-03-26T09:27:43.150813Z",
     "iopub.status.idle": "2024-03-26T09:27:49.041938Z",
     "shell.execute_reply": "2024-03-26T09:27:49.041033Z"
    },
    "papermill": {
     "duration": 5.902347,
     "end_time": "2024-03-26T09:27:49.044421",
     "exception": false,
     "start_time": "2024-03-26T09:27:43.142074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddc37ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:27:49.060515Z",
     "iopub.status.busy": "2024-03-26T09:27:49.060012Z",
     "iopub.status.idle": "2024-03-26T09:28:08.016158Z",
     "shell.execute_reply": "2024-03-26T09:28:08.015163Z"
    },
    "papermill": {
     "duration": 18.966493,
     "end_time": "2024-03-26T09:28:08.018281",
     "exception": false,
     "start_time": "2024-03-26T09:27:49.051788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((921497, 16, 20), (921497, 4), (921497,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "path = '/kaggle/input/calabi-yau-cicy-4-folds/'\n",
    "\n",
    "conf = np.load('/kaggle/input/calabi-yau-cicy-4-folds/conf.npy')\n",
    "hodge = np.load(os.path.join(path, 'hodge.npy'))\n",
    "direct = np.load(os.path.join(path, 'direct.npy'))\n",
    "conf.shape, hodge.shape, direct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a97db7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:08.035936Z",
     "iopub.status.busy": "2024-03-26T09:28:08.035145Z",
     "iopub.status.idle": "2024-03-26T09:28:08.039729Z",
     "shell.execute_reply": "2024-03-26T09:28:08.038908Z"
    },
    "papermill": {
     "duration": 0.015054,
     "end_time": "2024-03-26T09:28:08.041528",
     "exception": false,
     "start_time": "2024-03-26T09:28:08.026474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e259302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:08.057832Z",
     "iopub.status.busy": "2024-03-26T09:28:08.057501Z",
     "iopub.status.idle": "2024-03-26T09:28:08.177480Z",
     "shell.execute_reply": "2024-03-26T09:28:08.176410Z"
    },
    "papermill": {
     "duration": 0.130985,
     "end_time": "2024-03-26T09:28:08.179938",
     "exception": false,
     "start_time": "2024-03-26T09:28:08.048953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/calabi-yau-cicy-4-folds')\n",
    "from CICY4_functions import data_generator, batch_gd_scheduler,  calc_accuracy_mr, plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5c1e3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:08.196992Z",
     "iopub.status.busy": "2024-03-26T09:28:08.196640Z",
     "iopub.status.idle": "2024-03-26T09:28:08.441799Z",
     "shell.execute_reply": "2024-03-26T09:28:08.440920Z"
    },
    "papermill": {
     "duration": 0.256233,
     "end_time": "2024-03-26T09:28:08.444301",
     "exception": false,
     "start_time": "2024-03-26T09:28:08.188068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test(X, y):\n",
    "    X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101, shuffle = True)\n",
    "    \n",
    "    X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "    #only need reshape if the y dimension is 1\n",
    "    #y_train = torch.from_numpy(y_train.astype(np.float32).reshape(-1, 1))\n",
    "    y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "\n",
    "    X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "    #y_test = torch.from_numpy(y_test.astype(np.float32).reshape(-1, 1))\n",
    "    y_test = torch.from_numpy(y_test.astype(np.float32))                         \n",
    "    \n",
    "    print(f'X_train shape: {X_train.shape}, \\n y_train shape:{y_train.shape},\\\n",
    "                 \\n X_test shape: {X_test.shape}, \\n y_test shape:{y_test.shape}')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c963c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:08.471356Z",
     "iopub.status.busy": "2024-03-26T09:28:08.470935Z",
     "iopub.status.idle": "2024-03-26T09:28:09.842414Z",
     "shell.execute_reply": "2024-03-26T09:28:09.841282Z"
    },
    "papermill": {
     "duration": 1.384861,
     "end_time": "2024-03-26T09:28:09.844835",
     "exception": false,
     "start_time": "2024-03-26T09:28:08.459974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([737197, 16, 20]), \n",
      " y_train shape:torch.Size([737197, 4]),                 \n",
      " X_test shape: torch.Size([184300, 16, 20]), \n",
      " y_test shape:torch.Size([184300, 4])\n"
     ]
    }
   ],
   "source": [
    "X = conf\n",
    "y = hodge\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test(X, y)\n",
    "\n",
    "train_gen = lambda: data_generator(X_train, y_train)\n",
    "test_gen = lambda: data_generator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2301fb4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:09.863026Z",
     "iopub.status.busy": "2024-03-26T09:28:09.861874Z",
     "iopub.status.idle": "2024-03-26T09:28:09.918559Z",
     "shell.execute_reply": "2024-03-26T09:28:09.917586Z"
    },
    "papermill": {
     "duration": 0.067842,
     "end_time": "2024-03-26T09:28:09.920598",
     "exception": false,
     "start_time": "2024-03-26T09:28:09.852756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc69858",
   "metadata": {
    "papermill": {
     "duration": 0.007685,
     "end_time": "2024-03-26T09:28:09.936338",
     "exception": false,
     "start_time": "2024-03-26T09:28:09.928653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN-RNN hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e21fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:09.953283Z",
     "iopub.status.busy": "2024-03-26T09:28:09.952904Z",
     "iopub.status.idle": "2024-03-26T09:28:09.960489Z",
     "shell.execute_reply": "2024-03-26T09:28:09.959542Z"
    },
    "papermill": {
     "duration": 0.018282,
     "end_time": "2024-03-26T09:28:09.962448",
     "exception": false,
     "start_time": "2024-03-26T09:28:09.944166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################### CNN ###############################\n",
    "class CNN_block(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,128, 4, 1)\n",
    "        #self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128,64, 3, 1)\n",
    "        #self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.mxpool = nn.MaxPool2d(2,2)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.conv_total = nn.Sequential(\n",
    "            self.conv1,\n",
    "            #self.bn1,\n",
    "            self.mxpool,\n",
    "            #self.bn2,\n",
    "            self.conv2,\n",
    "            self.mxpool,\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv_total(x))\n",
    "        #reshape is the same as flat(x)\n",
    "        #x = x.reshape(x.shape[0], -1)\n",
    "        x = self.flat(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee86912d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:09.980025Z",
     "iopub.status.busy": "2024-03-26T09:28:09.979142Z",
     "iopub.status.idle": "2024-03-26T09:28:10.175290Z",
     "shell.execute_reply": "2024-03-26T09:28:10.174249Z"
    },
    "papermill": {
     "duration": 0.207445,
     "end_time": "2024-03-26T09:28:10.177551",
     "exception": false,
     "start_time": "2024-03-26T09:28:09.970106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_block(\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (mxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (conv_total): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_block = CNN_block()\n",
    "cnn_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "202db12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:10.195567Z",
     "iopub.status.busy": "2024-03-26T09:28:10.194762Z",
     "iopub.status.idle": "2024-03-26T09:28:10.204028Z",
     "shell.execute_reply": "2024-03-26T09:28:10.202979Z"
    },
    "papermill": {
     "duration": 0.020492,
     "end_time": "2024-03-26T09:28:10.206051",
     "exception": false,
     "start_time": "2024-03-26T09:28:10.185559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN_block(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):\n",
    "        super(RNN_block,self).__init__()\n",
    "        self.D = n_inputs\n",
    "        self.M = n_hidden\n",
    "        self.K = n_outputs\n",
    "        self.L = n_rnnlayers        \n",
    "        #self.lstm = nn.LSTM(input_size = self.D,\n",
    "        #                   hidden_size = self.M,\n",
    "        #                   num_layers = self.L,\n",
    "        #                   batch_first = True)    \n",
    "        self.gru = nn.GRU(input_size = self.D,\n",
    "                           hidden_size = self.M,\n",
    "                           num_layers = self.L,\n",
    "                           batch_first = True)\n",
    "        #self.fc1 = nn.Linear(self.M, 128)\n",
    "        #self.fc2 = nn.Linear(128, self.K)\n",
    "    def forward(self, X):\n",
    "        #input X is NxTxD\n",
    "        #initial hidden states\n",
    "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        #c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        #get LSTM unit output:\n",
    "        #output is NxTxM\n",
    "        #out, _ = self.lstm(X, (h0,c0))\n",
    "        out, _ = self.gru(X, h0)   \n",
    "        #we only want h(T) at the final time step\n",
    "        # output is now of shape (N, M)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0f90b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:10.224364Z",
     "iopub.status.busy": "2024-03-26T09:28:10.223628Z",
     "iopub.status.idle": "2024-03-26T09:28:10.372393Z",
     "shell.execute_reply": "2024-03-26T09:28:10.371300Z"
    },
    "papermill": {
     "duration": 0.160339,
     "end_time": "2024-03-26T09:28:10.374799",
     "exception": false,
     "start_time": "2024-03-26T09:28:10.214460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_block(\n",
       "  (gru): GRU(20, 256, num_layers=2, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gru_block = RNN_block(20, 64, 6, 4)\n",
    "#gru_block = RNN_block(20, 128, 4, 4)\n",
    "gru_block = RNN_block(20, 256, 2, 4)\n",
    "gru_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ccaa9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:10.393500Z",
     "iopub.status.busy": "2024-03-26T09:28:10.393104Z",
     "iopub.status.idle": "2024-03-26T09:28:10.401889Z",
     "shell.execute_reply": "2024-03-26T09:28:10.400827Z"
    },
    "papermill": {
     "duration": 0.020785,
     "end_time": "2024-03-26T09:28:10.404106",
     "exception": false,
     "start_time": "2024-03-26T09:28:10.383321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_RNN_hybrid(nn.Module):\n",
    "    def __init__(self, cnn_block, rnn_block, feat_vec_size):\n",
    "        super(CNN_RNN_hybrid, self).__init__()\n",
    "        self.cnn_block = cnn_block\n",
    "        self.rnn_block = rnn_block\n",
    "        self.feat_vec_size = feat_vec_size\n",
    "        self.fc1 = nn.Linear(self.feat_vec_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #output of cnn block is (N,384)\n",
    "        x1 = x.view(-1,1, 16,20)\n",
    "        x1 = self.cnn_block(x1)\n",
    "        #output of rnn block is (N,M = 64)\n",
    "        x2 = self.rnn_block(x)\n",
    "        #concatenate the 2 outputs to produce a feat vec (N, M+384)\n",
    "        xx = torch.cat([x1, x2], dim = 1)\n",
    "        # pass through linear layers\n",
    "        xx = self.fc1(xx)\n",
    "        #final output is 4\n",
    "        xx = self.fc2(xx)\n",
    "        \n",
    "        return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4076b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:10.423734Z",
     "iopub.status.busy": "2024-03-26T09:28:10.422775Z",
     "iopub.status.idle": "2024-03-26T09:28:10.440765Z",
     "shell.execute_reply": "2024-03-26T09:28:10.439646Z"
    },
    "papermill": {
     "duration": 0.02994,
     "end_time": "2024-03-26T09:28:10.442830",
     "exception": false,
     "start_time": "2024-03-26T09:28:10.412890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_RNN_hybrid(\n",
       "  (cnn_block): CNN_block(\n",
       "    (conv1): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (mxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "    (conv_total): Sequential(\n",
       "      (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (rnn_block): RNN_block(\n",
       "    (gru): GRU(20, 256, num_layers=2, batch_first=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=640, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_RNN_hybrid(cnn_block, gru_block, 256+384)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd1dacb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:10.461432Z",
     "iopub.status.busy": "2024-03-26T09:28:10.461023Z",
     "iopub.status.idle": "2024-03-26T09:28:10.466633Z",
     "shell.execute_reply": "2024-03-26T09:28:10.465743Z"
    },
    "papermill": {
     "duration": 0.017725,
     "end_time": "2024-03-26T09:28:10.468901",
     "exception": false,
     "start_time": "2024-03-26T09:28:10.451176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344708\n"
     ]
    }
   ],
   "source": [
    "#count the number of parameters in the model\n",
    "params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "print(sum(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013dc6b",
   "metadata": {
    "papermill": {
     "duration": 0.007974,
     "end_time": "2024-03-26T09:28:10.485051",
     "exception": false,
     "start_time": "2024-03-26T09:28:10.477077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Shape Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db47070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:10.503773Z",
     "iopub.status.busy": "2024-03-26T09:28:10.502741Z",
     "iopub.status.idle": "2024-03-26T09:28:10.632891Z",
     "shell.execute_reply": "2024-03-26T09:28:10.631454Z"
    },
    "papermill": {
     "duration": 0.141751,
     "end_time": "2024-03-26T09:28:10.635122",
     "exception": false,
     "start_time": "2024-03-26T09:28:10.493371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 16, 20])\n",
      "\n",
      "Shape of the image after passing through the GRU(20, 256, num_layers=2, batch_first=True): \n",
      " torch.Size([1, 16, 256])\n",
      "\n",
      "The final output shape is torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "y = y_train[0].to(device)\n",
    "print('RNN BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "h0 = torch.zeros(gru_block.L, X.size(0), gru_block.M).to(device)\n",
    "#c0 = torch.zeros(gru_block.L, X.size(0), gru_block.M).to(device)\n",
    "X, _ = gru_block.gru(X, h0)\n",
    "print(f'Shape of the image after passing through the {gru_block.gru}: \\n {X.shape}\\n')\n",
    "\n",
    "#get only the h(T) at the last time step\n",
    "Xg = X[:, -1, :]\n",
    "print(f'The final output shape is {Xg.shape}')\n",
    "#print(X)\n",
    "#print(f'Target: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b9adb0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:10.654090Z",
     "iopub.status.busy": "2024-03-26T09:28:10.653149Z",
     "iopub.status.idle": "2024-03-26T09:28:11.083707Z",
     "shell.execute_reply": "2024-03-26T09:28:11.082711Z"
    },
    "papermill": {
     "duration": 0.442513,
     "end_time": "2024-03-26T09:28:11.086074",
     "exception": false,
     "start_time": "2024-03-26T09:28:10.643561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 1, 16, 20])\n",
      "\n",
      "Original shape of the image after passing through Sequential(\n",
      "  (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "): \n",
      " torch.Size([1, 64, 2, 3])\n",
      "\n",
      "Original shape of the image after passing through Flatten(start_dim=1, end_dim=-1): \n",
      " torch.Size([1, 384])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "print('CNN BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "X = cnn_block.conv_total(X)\n",
    "print(f'Original shape of the image after passing through {cnn_block.conv_total}: \\n {X.shape}\\n')\n",
    "\n",
    "#X = X.reshape(X.shape[0], -1)\n",
    "#print(f'After reshaping: {X.shape}') [1,384]\n",
    "Xc = cnn_block.flat(X)\n",
    "print(f'Original shape of the image after passing through {cnn_block.flat}: \\n {Xc.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6bd10d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:11.104448Z",
     "iopub.status.busy": "2024-03-26T09:28:11.104111Z",
     "iopub.status.idle": "2024-03-26T09:28:11.134574Z",
     "shell.execute_reply": "2024-03-26T09:28:11.133627Z"
    },
    "papermill": {
     "duration": 0.042075,
     "end_time": "2024-03-26T09:28:11.136607",
     "exception": false,
     "start_time": "2024-03-26T09:28:11.094532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 384]), torch.Size([1, 640]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat = torch.cat([Xc, Xg], dim=1) \n",
    "Xg.shape, Xc.shape, X_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c23e167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:11.155874Z",
     "iopub.status.busy": "2024-03-26T09:28:11.155099Z",
     "iopub.status.idle": "2024-03-26T09:28:11.209918Z",
     "shell.execute_reply": "2024-03-26T09:28:11.208963Z"
    },
    "papermill": {
     "duration": 0.06675,
     "end_time": "2024-03-26T09:28:11.212201",
     "exception": false,
     "start_time": "2024-03-26T09:28:11.145451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-RNN HYBRID BLOCK SHAPE TRACING\n",
      "Original shape of the image before passing through the network: \n",
      " torch.Size([16, 20])\n",
      "\n",
      "Reshape the size to take in account the batch number\n",
      "The new size is torch.Size([1, 16, 20])\n",
      "\n",
      "Shape  after passing through the entire network: \n",
      " torch.Size([1, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X_train[0].to(device)\n",
    "print('CNN-RNN HYBRID BLOCK SHAPE TRACING')\n",
    "print(f'Original shape of the image before passing through the network: \\n {X.shape}\\n')\n",
    "\n",
    "print('Reshape the size to take in account the batch number')\n",
    "X = X.view(1,16,20)\n",
    "print(f'The new size is {X.shape}\\n')\n",
    "\n",
    "X = model(X)\n",
    "print(f'Shape  after passing through the entire network: \\n {X.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e5a05",
   "metadata": {
    "papermill": {
     "duration": 0.008407,
     "end_time": "2024-03-26T09:28:11.229334",
     "exception": false,
     "start_time": "2024-03-26T09:28:11.220927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train loop with scheduler\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html\n",
    "\n",
    "https://residentmario.github.io/pytorch-training-performance-guide/lr-sched-and-optim.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "092cfc04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:11.247795Z",
     "iopub.status.busy": "2024-03-26T09:28:11.247407Z",
     "iopub.status.idle": "2024-03-26T09:28:13.784199Z",
     "shell.execute_reply": "2024-03-26T09:28:13.783335Z"
    },
    "papermill": {
     "duration": 2.548655,
     "end_time": "2024-03-26T09:28:13.786519",
     "exception": false,
     "start_time": "2024-03-26T09:28:11.237864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epoch = 250\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 128\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.01)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', min_lr = 1e-6)\n",
    "#scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "#scheduler = lr_scheduler.OneCycleLR(optimizer=optimizer, epochs=max_epoch,\n",
    "#            pct_start=0.0, steps_per_epoch=len(X_train)//batch_size,\n",
    "#            max_lr= 0.01, div_factor=25, final_div_factor=4.0e-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dfb20a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T09:28:13.807250Z",
     "iopub.status.busy": "2024-03-26T09:28:13.806706Z",
     "iopub.status.idle": "2024-03-26T11:57:21.930220Z",
     "shell.execute_reply": "2024-03-26T11:57:21.929155Z"
    },
    "papermill": {
     "duration": 8948.163798,
     "end_time": "2024-03-26T11:57:21.959455",
     "exception": false,
     "start_time": "2024-03-26T09:28:13.795657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/250, train loss:  323.9169          test_loss:  453.5401, duration: 0:00:36.900387,           learning rate: (0.01, 0.01)\n",
      "Epoch: 2/250, train loss:  208.8667          test_loss:  208.9288, duration: 0:00:36.052691,           learning rate: (0.01, 0.01)\n",
      "Epoch: 3/250, train loss:  185.5889          test_loss:  219.6698, duration: 0:00:36.045686,           learning rate: (0.01, 0.01)\n",
      "Epoch: 4/250, train loss:  172.6976          test_loss:  156.1500, duration: 0:00:35.984200,           learning rate: (0.01, 0.01)\n",
      "Epoch: 5/250, train loss:  172.5973          test_loss:  168.0662, duration: 0:00:35.731218,           learning rate: (0.01, 0.01)\n",
      "Epoch: 6/250, train loss:  179.0316          test_loss:  170.8504, duration: 0:00:35.985929,           learning rate: (0.01, 0.01)\n",
      "Epoch: 7/250, train loss:  476.7500          test_loss:  809.3531, duration: 0:00:35.928424,           learning rate: (0.01, 0.01)\n",
      "Epoch: 8/250, train loss:  707.9016          test_loss:  561.1772, duration: 0:00:36.069123,           learning rate: (0.01, 0.01)\n",
      "Epoch: 9/250, train loss:  580.0415          test_loss:  559.5074, duration: 0:00:36.427303,           learning rate: (0.01, 0.01)\n",
      "Epoch: 10/250, train loss:  510.2048          test_loss:  502.4254, duration: 0:00:36.772439,           learning rate: (0.01, 0.01)\n",
      "Epoch: 11/250, train loss:  373.2575          test_loss:  337.5050, duration: 0:00:35.952200,           learning rate: (0.01, 0.01)\n",
      "Epoch: 12/250, train loss:  308.7530          test_loss:  255.7708, duration: 0:00:35.835676,           learning rate: (0.01, 0.01)\n",
      "Epoch: 13/250, train loss:  286.8572          test_loss:  231.1981, duration: 0:00:35.751935,           learning rate: (0.01, 0.01)\n",
      "Epoch: 14/250, train loss:  207.0738          test_loss:  148.7479, duration: 0:00:35.669173,           learning rate: (0.01, 0.01)\n",
      "Epoch: 15/250, train loss:  155.3563          test_loss:  133.5954, duration: 0:00:35.720169,           learning rate: (0.01, 0.01)\n",
      "Epoch: 16/250, train loss:  115.9401          test_loss:  99.5484, duration: 0:00:36.186276,           learning rate: (0.01, 0.01)\n",
      "Epoch: 17/250, train loss:  106.6556          test_loss:  98.5982, duration: 0:00:35.812362,           learning rate: (0.01, 0.01)\n",
      "Epoch: 18/250, train loss:  99.5635          test_loss:  138.9770, duration: 0:00:35.685238,           learning rate: (0.01, 0.01)\n",
      "Epoch: 19/250, train loss:  103.5685          test_loss:  83.4225, duration: 0:00:35.719889,           learning rate: (0.01, 0.01)\n",
      "Epoch: 20/250, train loss:  101.2600          test_loss:  84.8773, duration: 0:00:35.740229,           learning rate: (0.01, 0.01)\n",
      "Epoch: 21/250, train loss:  89.2503          test_loss:  69.8517, duration: 0:00:35.726910,           learning rate: (0.01, 0.01)\n",
      "Epoch: 22/250, train loss:  86.8171          test_loss:  58.3247, duration: 0:00:35.767158,           learning rate: (0.01, 0.01)\n",
      "Epoch: 23/250, train loss:  94.4470          test_loss:  72.2258, duration: 0:00:35.760952,           learning rate: (0.01, 0.01)\n",
      "Epoch: 24/250, train loss:  83.2133          test_loss:  82.8784, duration: 0:00:35.820839,           learning rate: (0.01, 0.01)\n",
      "Epoch: 25/250, train loss:  94.1021          test_loss:  90.9639, duration: 0:00:35.782986,           learning rate: (0.01, 0.01)\n",
      "Epoch: 26/250, train loss:  84.9974          test_loss:  73.5084, duration: 0:00:35.723054,           learning rate: (0.01, 0.01)\n",
      "Epoch: 27/250, train loss:  94.3628          test_loss:  84.4106, duration: 0:00:35.812179,           learning rate: (0.01, 0.01)\n",
      "Epoch: 28/250, train loss:  85.2772          test_loss:  86.2339, duration: 0:00:35.856719,           learning rate: (0.01, 0.01)\n",
      "Epoch: 29/250, train loss:  83.8617          test_loss:  69.0048, duration: 0:00:35.810661,           learning rate: (0.01, 0.01)\n",
      "Epoch: 30/250, train loss:  99.2787          test_loss:  101.9708, duration: 0:00:35.949359,           learning rate: (0.01, 0.01)\n",
      "Epoch: 31/250, train loss:  98.5028          test_loss:  73.3763, duration: 0:00:36.104797,           learning rate: (0.01, 0.01)\n",
      "Epoch: 32/250, train loss:  90.4143          test_loss:  85.1920, duration: 0:00:36.120950,           learning rate: (0.01, 0.01)\n",
      "Epoch: 33/250, train loss:  79.0390          test_loss:  67.3669, duration: 0:00:35.703737,           learning rate: (0.01, 0.001)\n",
      "Epoch: 34/250, train loss:  32.2916          test_loss:  28.6773, duration: 0:00:35.864851,           learning rate: (0.001, 0.001)\n",
      "Epoch: 35/250, train loss:  26.0119          test_loss:  25.2652, duration: 0:00:35.597284,           learning rate: (0.001, 0.001)\n",
      "Epoch: 36/250, train loss:  23.6850          test_loss:  24.6496, duration: 0:00:35.801329,           learning rate: (0.001, 0.001)\n",
      "Epoch: 37/250, train loss:  22.1117          test_loss:  22.0475, duration: 0:00:35.753703,           learning rate: (0.001, 0.001)\n",
      "Epoch: 38/250, train loss:  20.8019          test_loss:  23.4670, duration: 0:00:35.740398,           learning rate: (0.001, 0.001)\n",
      "Epoch: 39/250, train loss:  20.1638          test_loss:  21.4844, duration: 0:00:35.949424,           learning rate: (0.001, 0.001)\n",
      "Epoch: 40/250, train loss:  19.6763          test_loss:  20.4303, duration: 0:00:35.960011,           learning rate: (0.001, 0.001)\n",
      "Epoch: 41/250, train loss:  19.0857          test_loss:  19.0838, duration: 0:00:36.488998,           learning rate: (0.001, 0.001)\n",
      "Epoch: 42/250, train loss:  18.0178          test_loss:  18.3979, duration: 0:00:36.003029,           learning rate: (0.001, 0.001)\n",
      "Epoch: 43/250, train loss:  17.8164          test_loss:  20.0032, duration: 0:00:36.027763,           learning rate: (0.001, 0.001)\n",
      "Epoch: 44/250, train loss:  17.5025          test_loss:  17.0269, duration: 0:00:35.831520,           learning rate: (0.001, 0.001)\n",
      "Epoch: 45/250, train loss:  17.2249          test_loss:  19.6089, duration: 0:00:35.735263,           learning rate: (0.001, 0.001)\n",
      "Epoch: 46/250, train loss:  16.6415          test_loss:  17.4118, duration: 0:00:35.838657,           learning rate: (0.001, 0.001)\n",
      "Epoch: 47/250, train loss:  17.7462          test_loss:  17.2477, duration: 0:00:35.791641,           learning rate: (0.001, 0.001)\n",
      "Epoch: 48/250, train loss:  16.4190          test_loss:  18.7907, duration: 0:00:35.702356,           learning rate: (0.001, 0.001)\n",
      "Epoch: 49/250, train loss:  16.0338          test_loss:  16.9267, duration: 0:00:35.742622,           learning rate: (0.001, 0.001)\n",
      "Epoch: 50/250, train loss:  17.6848          test_loss:  17.9382, duration: 0:00:35.713540,           learning rate: (0.001, 0.001)\n",
      "Epoch: 51/250, train loss:  15.8734          test_loss:  15.3733, duration: 0:00:35.725156,           learning rate: (0.001, 0.001)\n",
      "Epoch: 52/250, train loss:  15.5474          test_loss:  16.3079, duration: 0:00:35.660930,           learning rate: (0.001, 0.001)\n",
      "Epoch: 53/250, train loss:  15.5487          test_loss:  16.1871, duration: 0:00:36.002400,           learning rate: (0.001, 0.001)\n",
      "Epoch: 54/250, train loss:  14.9278          test_loss:  14.4037, duration: 0:00:35.794283,           learning rate: (0.001, 0.001)\n",
      "Epoch: 55/250, train loss:  16.0768          test_loss:  18.8593, duration: 0:00:35.751834,           learning rate: (0.001, 0.001)\n",
      "Epoch: 56/250, train loss:  15.2109          test_loss:  15.8415, duration: 0:00:35.828988,           learning rate: (0.001, 0.001)\n",
      "Epoch: 57/250, train loss:  14.5196          test_loss:  14.5631, duration: 0:00:35.654308,           learning rate: (0.001, 0.001)\n",
      "Epoch: 58/250, train loss:  14.8408          test_loss:  14.1229, duration: 0:00:35.739992,           learning rate: (0.001, 0.001)\n",
      "Epoch: 59/250, train loss:  13.6885          test_loss:  13.9522, duration: 0:00:35.671221,           learning rate: (0.001, 0.001)\n",
      "Epoch: 60/250, train loss:  15.6556          test_loss:  14.6630, duration: 0:00:35.859460,           learning rate: (0.001, 0.001)\n",
      "Epoch: 61/250, train loss:  13.8106          test_loss:  14.1358, duration: 0:00:35.698315,           learning rate: (0.001, 0.001)\n",
      "Epoch: 62/250, train loss:  14.7419          test_loss:  15.2059, duration: 0:00:35.623109,           learning rate: (0.001, 0.001)\n",
      "Epoch: 63/250, train loss:  13.4932          test_loss:  14.2375, duration: 0:00:35.801924,           learning rate: (0.001, 0.001)\n",
      "Epoch: 64/250, train loss:  13.1187          test_loss:  14.3626, duration: 0:00:35.705245,           learning rate: (0.001, 0.001)\n",
      "Epoch: 65/250, train loss:  15.0656          test_loss:  13.8236, duration: 0:00:35.777197,           learning rate: (0.001, 0.001)\n",
      "Epoch: 66/250, train loss:  13.0635          test_loss:  16.1208, duration: 0:00:35.673492,           learning rate: (0.001, 0.001)\n",
      "Epoch: 67/250, train loss:  14.9478          test_loss:  12.6682, duration: 0:00:35.829855,           learning rate: (0.001, 0.001)\n",
      "Epoch: 68/250, train loss:  13.0346          test_loss:  13.5920, duration: 0:00:35.816353,           learning rate: (0.001, 0.001)\n",
      "Epoch: 69/250, train loss:  16.7515          test_loss:  16.8215, duration: 0:00:35.743355,           learning rate: (0.001, 0.001)\n",
      "Epoch: 70/250, train loss:  13.9290          test_loss:  14.7636, duration: 0:00:35.742877,           learning rate: (0.001, 0.001)\n",
      "Epoch: 71/250, train loss:  13.7685          test_loss:  15.6078, duration: 0:00:35.875869,           learning rate: (0.001, 0.001)\n",
      "Epoch: 72/250, train loss:  12.5739          test_loss:  13.8477, duration: 0:00:35.817475,           learning rate: (0.001, 0.001)\n",
      "Epoch: 73/250, train loss:  12.6860          test_loss:  12.9145, duration: 0:00:35.749115,           learning rate: (0.001, 0.001)\n",
      "Epoch: 74/250, train loss:  11.6509          test_loss:  11.4720, duration: 0:00:35.818246,           learning rate: (0.001, 0.001)\n",
      "Epoch: 75/250, train loss:  14.4498          test_loss:  22.8597, duration: 0:00:35.703629,           learning rate: (0.001, 0.001)\n",
      "Epoch: 76/250, train loss:  10.9420          test_loss:  12.7629, duration: 0:00:35.879402,           learning rate: (0.001, 0.001)\n",
      "Epoch: 77/250, train loss:  11.6817          test_loss:  16.4973, duration: 0:00:35.582210,           learning rate: (0.001, 0.001)\n",
      "Epoch: 78/250, train loss:  14.1874          test_loss:  12.3860, duration: 0:00:35.709933,           learning rate: (0.001, 0.001)\n",
      "Epoch: 79/250, train loss:  13.3102          test_loss:  12.9125, duration: 0:00:35.706925,           learning rate: (0.001, 0.001)\n",
      "Epoch: 80/250, train loss:  10.3874          test_loss:  13.6674, duration: 0:00:35.660512,           learning rate: (0.001, 0.001)\n",
      "Epoch: 81/250, train loss:  13.6264          test_loss:  10.6648, duration: 0:00:35.840758,           learning rate: (0.001, 0.001)\n",
      "Epoch: 82/250, train loss:  11.1144          test_loss:  13.7753, duration: 0:00:35.761134,           learning rate: (0.001, 0.001)\n",
      "Epoch: 83/250, train loss:  10.4708          test_loss:  13.4551, duration: 0:00:35.700514,           learning rate: (0.001, 0.001)\n",
      "Epoch: 84/250, train loss:  11.1677          test_loss:  11.6708, duration: 0:00:35.691820,           learning rate: (0.001, 0.001)\n",
      "Epoch: 85/250, train loss:  12.1640          test_loss:  22.2823, duration: 0:00:35.708685,           learning rate: (0.001, 0.001)\n",
      "Epoch: 86/250, train loss:  10.4693          test_loss:  11.2623, duration: 0:00:35.653393,           learning rate: (0.001, 0.001)\n",
      "Epoch: 87/250, train loss:  11.8661          test_loss:  10.1141, duration: 0:00:35.736400,           learning rate: (0.001, 0.001)\n",
      "Epoch: 88/250, train loss:  13.8301          test_loss:  11.9692, duration: 0:00:35.776198,           learning rate: (0.001, 0.001)\n",
      "Epoch: 89/250, train loss:  13.3607          test_loss:  24.8501, duration: 0:00:35.775584,           learning rate: (0.001, 0.001)\n",
      "Epoch: 90/250, train loss:  12.2841          test_loss:  10.1732, duration: 0:00:35.792770,           learning rate: (0.001, 0.001)\n",
      "Epoch: 91/250, train loss:  10.6844          test_loss:  14.0752, duration: 0:00:35.731335,           learning rate: (0.001, 0.001)\n",
      "Epoch: 92/250, train loss:  12.6761          test_loss:  14.9373, duration: 0:00:35.756776,           learning rate: (0.001, 0.001)\n",
      "Epoch: 93/250, train loss:  12.2031          test_loss:  11.5556, duration: 0:00:35.771017,           learning rate: (0.001, 0.001)\n",
      "Epoch: 94/250, train loss:  13.1627          test_loss:  12.1497, duration: 0:00:35.744902,           learning rate: (0.001, 0.001)\n",
      "Epoch: 95/250, train loss:  10.6233          test_loss:  9.7164, duration: 0:00:35.806350,           learning rate: (0.001, 0.001)\n",
      "Epoch: 96/250, train loss:  11.5125          test_loss:  9.3027, duration: 0:00:35.939115,           learning rate: (0.001, 0.001)\n",
      "Epoch: 97/250, train loss:  10.4511          test_loss:  11.1522, duration: 0:00:35.728431,           learning rate: (0.001, 0.001)\n",
      "Epoch: 98/250, train loss:  11.2859          test_loss:  9.7429, duration: 0:00:35.747624,           learning rate: (0.001, 0.001)\n",
      "Epoch: 99/250, train loss:  9.3490          test_loss:  10.4437, duration: 0:00:35.706906,           learning rate: (0.001, 0.001)\n",
      "Epoch: 100/250, train loss:  13.3313          test_loss:  11.1694, duration: 0:00:35.793427,           learning rate: (0.001, 0.001)\n",
      "Epoch: 101/250, train loss:  9.9971          test_loss:  12.2971, duration: 0:00:35.733366,           learning rate: (0.001, 0.001)\n",
      "Epoch: 102/250, train loss:  13.1127          test_loss:  11.2346, duration: 0:00:35.891817,           learning rate: (0.001, 0.001)\n",
      "Epoch: 103/250, train loss:  9.8054          test_loss:  10.2603, duration: 0:00:35.913578,           learning rate: (0.001, 0.001)\n",
      "Epoch: 104/250, train loss:  9.4415          test_loss:  10.0598, duration: 0:00:35.838880,           learning rate: (0.001, 0.001)\n",
      "Epoch: 105/250, train loss:  11.6054          test_loss:  10.9843, duration: 0:00:35.698841,           learning rate: (0.001, 0.001)\n",
      "Epoch: 106/250, train loss:  11.1910          test_loss:  11.4698, duration: 0:00:35.759282,           learning rate: (0.001, 0.001)\n",
      "Epoch: 107/250, train loss:  10.1774          test_loss:  11.0950, duration: 0:00:35.692072,           learning rate: (0.001, 0.0001)\n",
      "Epoch: 108/250, train loss:  5.0574          test_loss:  6.4222, duration: 0:00:35.783820,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 109/250, train loss:  4.4559          test_loss:  6.0903, duration: 0:00:35.756183,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 110/250, train loss:  4.2552          test_loss:  5.9583, duration: 0:00:35.831856,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 111/250, train loss:  4.0973          test_loss:  5.6724, duration: 0:00:35.816468,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 112/250, train loss:  3.9977          test_loss:  5.6100, duration: 0:00:35.792108,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 113/250, train loss:  3.9144          test_loss:  5.5180, duration: 0:00:35.819081,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 114/250, train loss:  3.8348          test_loss:  5.4391, duration: 0:00:35.812077,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 115/250, train loss:  3.7693          test_loss:  5.4333, duration: 0:00:35.796030,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 116/250, train loss:  3.7143          test_loss:  5.3360, duration: 0:00:35.975145,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 117/250, train loss:  3.6565          test_loss:  5.3669, duration: 0:00:35.928649,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 118/250, train loss:  3.6092          test_loss:  5.2903, duration: 0:00:35.732370,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 119/250, train loss:  3.5640          test_loss:  5.2662, duration: 0:00:35.683520,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 120/250, train loss:  3.5156          test_loss:  5.3275, duration: 0:00:35.672084,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 121/250, train loss:  3.4731          test_loss:  5.3218, duration: 0:00:35.783944,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 122/250, train loss:  3.4363          test_loss:  5.1008, duration: 0:00:35.717244,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 123/250, train loss:  3.3993          test_loss:  5.1438, duration: 0:00:35.899957,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 124/250, train loss:  3.3605          test_loss:  5.0158, duration: 0:00:36.227822,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 125/250, train loss:  3.3292          test_loss:  5.0154, duration: 0:00:35.897000,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 126/250, train loss:  3.2976          test_loss:  5.1602, duration: 0:00:35.723417,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 127/250, train loss:  3.2634          test_loss:  4.9004, duration: 0:00:35.793578,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 128/250, train loss:  3.2380          test_loss:  4.9249, duration: 0:00:35.837186,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 129/250, train loss:  3.2037          test_loss:  4.8561, duration: 0:00:35.787032,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 130/250, train loss:  3.1841          test_loss:  4.8355, duration: 0:00:35.726279,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 131/250, train loss:  3.1528          test_loss:  4.8797, duration: 0:00:35.751242,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 132/250, train loss:  3.1280          test_loss:  4.8234, duration: 0:00:35.689289,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 133/250, train loss:  3.1076          test_loss:  4.7769, duration: 0:00:35.675220,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 134/250, train loss:  3.0799          test_loss:  4.7743, duration: 0:00:35.771708,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 135/250, train loss:  3.0512          test_loss:  4.7944, duration: 0:00:35.562605,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 136/250, train loss:  3.0287          test_loss:  4.7658, duration: 0:00:35.845501,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 137/250, train loss:  3.0188          test_loss:  4.6192, duration: 0:00:35.749127,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 138/250, train loss:  2.9897          test_loss:  4.6834, duration: 0:00:35.824333,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 139/250, train loss:  2.9682          test_loss:  4.5829, duration: 0:00:35.664152,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 140/250, train loss:  2.9470          test_loss:  4.6112, duration: 0:00:35.726117,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 141/250, train loss:  2.9324          test_loss:  4.6657, duration: 0:00:36.225083,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 142/250, train loss:  2.9145          test_loss:  4.6180, duration: 0:00:35.853234,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 143/250, train loss:  2.8845          test_loss:  4.6428, duration: 0:00:35.750507,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 144/250, train loss:  2.8679          test_loss:  4.6264, duration: 0:00:35.646996,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 145/250, train loss:  2.8531          test_loss:  4.6211, duration: 0:00:35.764108,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 146/250, train loss:  2.8347          test_loss:  4.4962, duration: 0:00:35.772861,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 147/250, train loss:  2.8221          test_loss:  4.5115, duration: 0:00:35.824239,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 148/250, train loss:  2.8084          test_loss:  4.5068, duration: 0:00:35.650154,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 149/250, train loss:  2.7892          test_loss:  4.4814, duration: 0:00:35.767336,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 150/250, train loss:  2.7708          test_loss:  4.5656, duration: 0:00:35.745501,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 151/250, train loss:  2.7546          test_loss:  4.4670, duration: 0:00:35.785680,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 152/250, train loss:  2.7425          test_loss:  4.5277, duration: 0:00:35.737433,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 153/250, train loss:  2.7357          test_loss:  4.5105, duration: 0:00:35.773023,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 154/250, train loss:  2.7109          test_loss:  4.4094, duration: 0:00:35.682783,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 155/250, train loss:  2.7024          test_loss:  4.4234, duration: 0:00:35.770751,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 156/250, train loss:  2.6820          test_loss:  4.5076, duration: 0:00:35.693850,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 157/250, train loss:  2.6721          test_loss:  4.4301, duration: 0:00:35.847104,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 158/250, train loss:  2.6550          test_loss:  4.4012, duration: 0:00:35.902076,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 159/250, train loss:  2.6488          test_loss:  4.3649, duration: 0:00:35.724220,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 160/250, train loss:  2.6367          test_loss:  4.3785, duration: 0:00:35.876198,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 161/250, train loss:  2.6208          test_loss:  4.6768, duration: 0:00:35.937552,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 162/250, train loss:  2.6086          test_loss:  4.4709, duration: 0:00:35.732482,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 163/250, train loss:  2.6006          test_loss:  4.3024, duration: 0:00:35.761448,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 164/250, train loss:  2.5963          test_loss:  4.5281, duration: 0:00:35.649943,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 165/250, train loss:  2.5702          test_loss:  4.3228, duration: 0:00:35.642480,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 166/250, train loss:  2.5875          test_loss:  4.3906, duration: 0:00:35.737521,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 167/250, train loss:  2.5479          test_loss:  4.3539, duration: 0:00:35.809805,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 168/250, train loss:  2.5473          test_loss:  4.3383, duration: 0:00:35.671006,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 169/250, train loss:  2.5315          test_loss:  4.4065, duration: 0:00:35.513932,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 170/250, train loss:  2.5489          test_loss:  4.3924, duration: 0:00:35.659082,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 171/250, train loss:  2.5068          test_loss:  4.2736, duration: 0:00:35.676369,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 172/250, train loss:  2.5096          test_loss:  4.2692, duration: 0:00:35.671889,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 173/250, train loss:  2.4893          test_loss:  4.2397, duration: 0:00:35.735814,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 174/250, train loss:  2.4887          test_loss:  4.3502, duration: 0:00:35.809007,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 175/250, train loss:  2.4754          test_loss:  4.4733, duration: 0:00:35.747326,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 176/250, train loss:  2.4655          test_loss:  4.2937, duration: 0:00:35.751223,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 177/250, train loss:  2.4559          test_loss:  4.1890, duration: 0:00:35.635036,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 178/250, train loss:  2.4555          test_loss:  4.2140, duration: 0:00:35.657410,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 179/250, train loss:  2.4318          test_loss:  4.3374, duration: 0:00:35.627270,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 180/250, train loss:  2.4314          test_loss:  4.1348, duration: 0:00:35.833497,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 181/250, train loss:  2.4330          test_loss:  4.2451, duration: 0:00:35.736143,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 182/250, train loss:  2.4047          test_loss:  4.2565, duration: 0:00:35.810843,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 183/250, train loss:  2.4153          test_loss:  4.2008, duration: 0:00:36.409959,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 184/250, train loss:  2.3937          test_loss:  4.3087, duration: 0:00:35.804265,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 185/250, train loss:  2.3972          test_loss:  4.5061, duration: 0:00:36.015046,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 186/250, train loss:  2.3839          test_loss:  4.2079, duration: 0:00:35.695483,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 187/250, train loss:  2.4114          test_loss:  4.2222, duration: 0:00:35.843308,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 188/250, train loss:  2.3806          test_loss:  4.1982, duration: 0:00:35.710695,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 189/250, train loss:  2.3686          test_loss:  4.2263, duration: 0:00:35.738852,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 190/250, train loss:  2.3708          test_loss:  4.2323, duration: 0:00:35.728264,           learning rate: (0.0001, 0.0001)\n",
      "Epoch: 191/250, train loss:  2.3439          test_loss:  4.2580, duration: 0:00:35.610469,           learning rate: (0.0001, 1e-05)\n",
      "Epoch: 192/250, train loss:  1.9341          test_loss:  3.8548, duration: 0:00:35.766703,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 193/250, train loss:  1.9028          test_loss:  3.8377, duration: 0:00:35.632197,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 194/250, train loss:  1.8940          test_loss:  3.8438, duration: 0:00:35.607336,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 195/250, train loss:  1.8894          test_loss:  3.8244, duration: 0:00:35.907069,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 196/250, train loss:  1.8854          test_loss:  3.8279, duration: 0:00:35.929998,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 197/250, train loss:  1.8813          test_loss:  3.8310, duration: 0:00:35.537911,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 198/250, train loss:  1.8783          test_loss:  3.8284, duration: 0:00:35.649097,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 199/250, train loss:  1.8756          test_loss:  3.8157, duration: 0:00:35.864491,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 200/250, train loss:  1.8735          test_loss:  3.8314, duration: 0:00:35.693418,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 201/250, train loss:  1.8714          test_loss:  3.8282, duration: 0:00:35.628220,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 202/250, train loss:  1.8698          test_loss:  3.8386, duration: 0:00:35.629071,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 203/250, train loss:  1.8671          test_loss:  3.8210, duration: 0:00:35.835411,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 204/250, train loss:  1.8651          test_loss:  3.8169, duration: 0:00:35.787645,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 205/250, train loss:  1.8626          test_loss:  3.8209, duration: 0:00:35.569182,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 206/250, train loss:  1.8608          test_loss:  3.8316, duration: 0:00:35.703183,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 207/250, train loss:  1.8589          test_loss:  3.8153, duration: 0:00:35.774896,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 208/250, train loss:  1.8563          test_loss:  3.8460, duration: 0:00:35.744527,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 209/250, train loss:  1.8548          test_loss:  3.8127, duration: 0:00:35.729386,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 210/250, train loss:  1.8527          test_loss:  3.8170, duration: 0:00:35.976753,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 211/250, train loss:  1.8514          test_loss:  3.8250, duration: 0:00:35.781459,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 212/250, train loss:  1.8492          test_loss:  3.8023, duration: 0:00:35.898946,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 213/250, train loss:  1.8480          test_loss:  3.7959, duration: 0:00:35.658204,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 214/250, train loss:  1.8460          test_loss:  3.8012, duration: 0:00:35.786579,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 215/250, train loss:  1.8439          test_loss:  3.8151, duration: 0:00:35.698903,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 216/250, train loss:  1.8428          test_loss:  3.7977, duration: 0:00:35.896592,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 217/250, train loss:  1.8402          test_loss:  3.8203, duration: 0:00:36.078493,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 218/250, train loss:  1.8395          test_loss:  3.7929, duration: 0:00:35.784890,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 219/250, train loss:  1.8374          test_loss:  3.8022, duration: 0:00:35.676961,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 220/250, train loss:  1.8364          test_loss:  3.7963, duration: 0:00:35.664060,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 221/250, train loss:  1.8338          test_loss:  3.8036, duration: 0:00:35.757793,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 222/250, train loss:  1.8326          test_loss:  3.7828, duration: 0:00:35.730386,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 223/250, train loss:  1.8319          test_loss:  3.7876, duration: 0:00:35.656457,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 224/250, train loss:  1.8307          test_loss:  3.7856, duration: 0:00:35.865087,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 225/250, train loss:  1.8292          test_loss:  3.7742, duration: 0:00:35.581673,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 226/250, train loss:  1.8266          test_loss:  3.7749, duration: 0:00:35.652513,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 227/250, train loss:  1.8260          test_loss:  3.8118, duration: 0:00:35.692792,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 228/250, train loss:  1.8241          test_loss:  3.7776, duration: 0:00:35.693036,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 229/250, train loss:  1.8217          test_loss:  3.7798, duration: 0:00:35.772602,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 230/250, train loss:  1.8204          test_loss:  3.7727, duration: 0:00:35.556912,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 231/250, train loss:  1.8194          test_loss:  3.7800, duration: 0:00:35.706762,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 232/250, train loss:  1.8188          test_loss:  3.7565, duration: 0:00:35.726056,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 233/250, train loss:  1.8164          test_loss:  3.7845, duration: 0:00:35.655205,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 234/250, train loss:  1.8154          test_loss:  3.8446, duration: 0:00:35.669201,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 235/250, train loss:  1.8141          test_loss:  3.7750, duration: 0:00:35.660187,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 236/250, train loss:  1.8129          test_loss:  3.7670, duration: 0:00:35.604987,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 237/250, train loss:  1.8109          test_loss:  3.7552, duration: 0:00:35.961433,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 238/250, train loss:  1.8097          test_loss:  3.7773, duration: 0:00:35.906134,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 239/250, train loss:  1.8090          test_loss:  3.7546, duration: 0:00:35.684697,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 240/250, train loss:  1.8079          test_loss:  3.7596, duration: 0:00:35.636639,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 241/250, train loss:  1.8055          test_loss:  3.7566, duration: 0:00:35.762983,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 242/250, train loss:  1.8036          test_loss:  3.7413, duration: 0:00:35.765187,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 243/250, train loss:  1.8020          test_loss:  3.7447, duration: 0:00:35.717382,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 244/250, train loss:  1.8013          test_loss:  3.7623, duration: 0:00:35.760598,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 245/250, train loss:  1.8000          test_loss:  3.7481, duration: 0:00:35.696159,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 246/250, train loss:  1.7992          test_loss:  3.7393, duration: 0:00:35.907578,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 247/250, train loss:  1.7978          test_loss:  3.7601, duration: 0:00:35.684563,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 248/250, train loss:  1.7965          test_loss:  3.7422, duration: 0:00:35.808432,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 249/250, train loss:  1.7952          test_loss:  3.7366, duration: 0:00:35.704726,           learning rate: (1e-05, 1e-05)\n",
      "Epoch: 250/250, train loss:  1.7936          test_loss:  3.7886, duration: 0:00:36.260245,           learning rate: (1e-05, 1e-05)\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses=batch_gd_scheduler(model, criterion, optimizer, train_gen, test_gen, scheduler, \n",
    "                                             max_epoch, device,  cnn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5500b687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T11:57:22.014716Z",
     "iopub.status.busy": "2024-03-26T11:57:22.014394Z",
     "iopub.status.idle": "2024-03-26T11:57:38.857238Z",
     "shell.execute_reply": "2024-03-26T11:57:38.856247Z"
    },
    "papermill": {
     "duration": 16.8732,
     "end_time": "2024-03-26T11:57:38.859567",
     "exception": false,
     "start_time": "2024-03-26T11:57:21.986367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for h11:0.8479, Test accuracy for h11: 0.8434\n",
      "Train accuracy for h21:0.7523, Test accuracy for h21: 0.7426\n",
      "Train accuracy for h31:0.5719, Test accuracy for h31: 0.5254\n",
      "Train accuracy for h22:0.2230, Test accuracy for h22: 0.1979\n"
     ]
    }
   ],
   "source": [
    "train_acc, test_acc = calc_accuracy_mr(model, train_gen , test_gen, device = device, cnn= False)\n",
    "print(f'Train accuracy for h11:{train_acc[0]:.4f}, Test accuracy for h11: {test_acc[0]:.4f}')\n",
    "print(f'Train accuracy for h21:{train_acc[1]:.4f}, Test accuracy for h21: {test_acc[1]:.4f}')\n",
    "print(f'Train accuracy for h31:{train_acc[2]:.4f}, Test accuracy for h31: {test_acc[2]:.4f}')\n",
    "print(f'Train accuracy for h22:{train_acc[3]:.4f}, Test accuracy for h22: {test_acc[3]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48753611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T11:57:38.915729Z",
     "iopub.status.busy": "2024-03-26T11:57:38.915403Z",
     "iopub.status.idle": "2024-03-26T11:57:38.932410Z",
     "shell.execute_reply": "2024-03-26T11:57:38.931500Z"
    },
    "papermill": {
     "duration": 0.047106,
     "end_time": "2024-03-26T11:57:38.934392",
     "exception": false,
     "start_time": "2024-03-26T11:57:38.887286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, '/kaggle/working/saved_models/CNN_GRU_hybrid_cicy4_Hodge_v2.pt')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4575883,
     "sourceId": 7936105,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9001.013485,
   "end_time": "2024-03-26T11:57:41.198479",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-26T09:27:40.184994",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
